# Linear DAEs with Constant Coefficients

## Basic Notions and Definitions

Consider the DAE in the form
\begin{equation}
E \dot x (t) = Ax(t) + f(t), (\#eq:gen-lin-cc-dae)
\end{equation}
where $E$, $A\in \mathbb R^{m,n}$ and $f\in \mathcal C(\mathbb I, \mathbb R^m)$ with, possibly, an initial condition
\begin{equation}
x(t_0) = x_0 \in \mathbb R^{n}. (\#eq:gen-lin-cc-dae-inic)
\end{equation}

```{block, type='JHSAYS'}
For utmost generality, we consider the case that $m\neq n$, i.e. the number of equations does not meet the number of unknowns, but we will turn to the square case of $m=n$ soon.
```

### Scalings and State Transformations

One can confirm that if $x$ is a solution to \@ref(eq:gen-lin-cc-dae) and $P\in \mathbb R^{n,n}$ is invertible, then $x$ is a solution to
\begin{equation*}
PE \dot x (t) = PAx(t) + Pf(t).
\end{equation*}

```{block, type='JHSAYS'}
This is a scaling of the equations.
```

Similarly, if $Q\in \mathbb R^{n,n}$ is invertible, then $\tilde x := Q^{-1}x$ solves

\begin{equation*}
E Q \dot {\tilde x} (t) = AQ \tilde x(t) + f(t).
\end{equation*}

```{block, type='JHSAYS'}
This is a state transformation of the system.
```
Thus, when talking of solvability of \@ref(eq:gen-lin-cc-dae), one may equivalently consider any regular $Q\in \mathbb R^{m,m}$, $P\in \mathbb R^{m,m}$ and the scaled and transformed system
\begin{equation}
\tilde E \dot{\tilde x}(t) = \tilde A \tilde x (t) + \tilde f(t), \quad \tilde x(0) = Q^{-1}x_0,
\end{equation}
where $\tilde E = PEQ$, $\tilde A = PAQ$, $\tilde f = Pf$, and $x=Q\tilde x$.

To characterize all scalings and state transformations, we define these operations as relations of matrix pairs:


```{definition, label="matrix-pair-equivalent"}
Two pairs of matrices $(E_1, A_1)$ and $(E_2, A_2$, $E_1$, $A_1$, $E_2$, $A_2 \in \mathbb R^{m,n}$ are called *strongly equivalent*, if there exist regular matrices $P\in \mathbb R^{m,m}$, $Q\in \mathbb R^{n,n}$ such that
\begin{equation*}
E_2 = PE_1Q, \quad A_2 = PA_1Q.
\end{equation*}
In this case, we write 
$$(E_1, A_1) \sim (E_2, A_2).$$
```

```{lemma, label="strong-equivalence-RSI"}
The relation $\sim$ defined in Definition \@ref(def:matrix-pair-equivalent) defines an equivalence relation[^1].
```
```{proof}
Exercise.
```

[^1]: Equivalence relation -- **RST**. **R**eflexive: $A\sim A$. **S**ymmetric: $A\sim B$, then $B\sim A$. **T**ransitive: $A\sim B$ and $B\sim C$, then $A\sim C$.

```{definition, label="drazin-inverse"}

Let $E\in \mathbb C^{n,n}$ and $\nu = \ind(E)$. A matrix $X\in \mathbb C^{n,n}$ that fulfills 

\begin{align}
	EX & = XE, (\#eq:def-drazin-a) \\
	XEX & = X, (\#eq:def-drazin-b) \\
	XE^{\nu+1} & = E^{\nu}, (\#eq:def-drazin-c)
\end{align}

is called a *Drazin inverse* of $E$.
```

With the following theorem we confirm that a Drazin inverse to a matrix $E$ is unique so that we can write $E^D$ for it.

```{theorem, label="drazin-inverse-unique"}

Every matrix $E\in\mathbb{C}^{n,n}$ has one, and only one, Drazin inverse.

```

```{proof}
Uniqueness: Let $X_1$ and $X_2$ be two Drazin inverses of $E$. Then by repeated application of the identities in \@ref(eq:def-drazin-a)--\@ref(eq:def-drazin-c) one derives that 

\begin{align*}
X_1 EX_1 E X_2 =   &X_1EX_2 = X_1EX_2EX_2  \\
X_1^2 E^2 X_2 = \dotsm=   &X_1EX_2 = \dotsm= X_1E^2X_2^2  \\
X_1^{\nu+1}E^{\nu+1} X_2 =\dotsm=\dotsm=   &X_1EX_2 =\dotsm=\dotsm= X_1E^{\nu+1}X_2^{\nu+1}  \\
X_1^{\nu+1}E^{\nu+1} X_1 =\dotsm=\dotsm=\dotsm=   &X_1EX_2 =\dotsm=\dotsm=\dotsm= X_2E^{\nu+1}X_2^{\nu+1}  \\
X_1 =\dotsm=\dotsm=\dotsm=\dotsm=   &X_1EX_2 =\dotsm=\dotsm=\dotsm=\dotsm= X_2, \\
\end{align*}

where in the second last step we used the identities

$$
	E^{\nu+1}X_1=X_1E^{\nu+1}=E^{\nu}=X_2E^{\nu+1}=E^{\nu+1}X_2.
$$
```
