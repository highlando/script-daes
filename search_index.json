[["index.html", "DAEs Preface", " DAEs Jan Heiland 2021-04-29 Preface This is a writeup of the introduction and, maybe, some other aspects of my lectures on DAEs at the OVGU Magdeburg. Fixes and feature requests can be submitted to the github-repo. "],["introduction.html", "1 Introduction 1.1 Examples 1.2 Why are DAEs difficult to treat", " 1 Introduction Differential-algebraic equations (DAEs) are coupled differential- and algebraic equations. DAEs often describe dynamical processes – here are the differential equations – that are subject to constraints: the algebraic equations. Let’s start with a few examples. 1.1 Examples Free fall vs. the pendulum Figure 1.1: Free fall of a point mass. Here, the laws of the free fall – a special case of Newton’s second law – applies: force equals mass times acceleration In 2D, the \\(x\\), \\(y\\) coordinates of a point of mass \\(m\\): \\[\\begin{align*} m\\ddot x &amp;= 0 \\\\ m\\ddot y &amp;= -mg \\end{align*}\\] where \\(g\\) is the gravity; see Figure 1.1. The Pendulum The same point mass attached to a string. Figure 1.2: A pendulum. Again, we have force = mass*acceleration but also the conditions that the mass moves on a circle: \\[ (x(t) - c_x)^2 + (y(t) - c_y)^2 = l^2, \\] where \\((c_x, c_y)\\) are the coordinates of the center and \\(l\\) is the length of the string; see Figure 1.2. We use the Lagrangian function to derive the Euler-Lagrange equations of motion. For the pendulum, we have the kinetic energy \\[ T = \\frac 12 m (\\dot x(t)^2 + \\dot y(t)^2), \\] the potential \\[ U = mgy, \\] and the constraint \\[ h = (x(t) - c_x)^2 + (y(t) - c_y)^2 - l^2 . \\] Thus, with \\[L:=U-T- \\lambda h\\] and the requirement that \\[ \\frac{d}{dt}(\\frac{\\partial L}{\\partial \\dot q}) - \\frac{\\partial L}{\\partial q} = 0 \\] for all of the generalized coordinates \\(q=x\\), \\(y\\), \\(\\lambda\\), one obtains a system of equations: Generalized coordinate Equation \\(q \\leftarrow x\\) \\(m\\ddot x(t) + 2 \\lambda(t) (x(t) - c_x) = 0\\) \\(q \\leftarrow y\\) \\(m\\ddot y(t) + mgy + 2 \\lambda(t) (y(t) - c_y) = 0\\) \\(q \\leftarrow \\lambda\\) \\((x(t) - c_x)^2 + (y(t) - c_y)^2 - l^2 =0\\) Example 1.1 (The Pendulum) After an order reduction via the new variables \\(u:=\\dot x\\) and \\(v=\\dot y\\) the overall system reads \\[\\begin{equation} \\begin{split} \\dot x &amp;= u \\\\ \\dot y &amp;= v \\\\ m \\dot u &amp;= - 2 \\lambda (x - c_x) \\\\ m \\dot v &amp;= - 2 \\lambda (y - c_y) - mgy \\\\ 0&amp;=(x - c_x)^2 + (y - c_y)^2 - l^2, \\end{split} \\tag{1.1} \\end{equation}\\] where we have omitted the time dependence. Equation (1.1) is a canonical example for a DAE with combined differential and algebraic equations. Electrical Circuits Another class of DAEs arises from the modelling electrical circuits. We consider the example of charging a conductor through a resistor as illustrated in Figure 1.3. Figure 1.3: Electrical circuit with a source, a resistor, and a conductor. We formulate the problem in terms of the potentials \\(x_1\\), \\(x_2\\), \\(x_3\\), that are assumed to reside in the wires between a source \\(U\\) and a resistor \\(R\\), the resistor \\(R\\) and the capacitor \\(C\\), and the capacitor and the source. A model for the circuit is given through the following principles and considerations. Model principle Equation The source defines the difference in the neighboring potentials: \\(x_1 - x_3 - U = 0\\) The current \\(I_R\\) that is induced by the potentials neighboring the resistor is is defined through Ohm’s law: \\(I_R = \\frac{x_1 - x_2}{R}\\) The current \\(I_C\\) that is induced by the potentials neighboring the capacitor is described through: \\(I_C = C(\\dot x_3 - \\dot x_2)\\) Everywhere in the circuit the currents sum up to zero. (This is Kirchhoff’s law): \\(I_C + I_R = C(\\dot x_3 - \\dot x_2)+ \\frac{x_1 - x_2}{R}=0\\) To fix the potential, one can set a ground potential – here we choose \\(x_3\\). (note that so far all equations only consider differences in the potential). \\(x_3 = 0\\) Example 1.2 Summing all up, the equations that model the circuit are given as \\[\\begin{equation} \\begin{split} C(\\dot x_3 - \\dot x_2) &amp;= - \\frac{x_1 - x_2}{R} \\\\ 0 &amp;= x_1 - x_3 - U \\\\ 0 &amp;= x_3. \\end{split} \\tag{1.2} \\end{equation}\\] Navier-Stokes Equations The Navier-Stokes equations (NSE) are commonly used to model all kind of flows. They describe the evolution of the velocity \\(v\\) of the fluid and the pressure \\(p\\) in the fluid. Note that the flow occupies a spatial domain, say in \\(\\mathbb R^{3}\\) so that \\(v\\) and \\(p\\) are functions both of the time variable \\(t\\) and a space variable \\(\\xi\\): \\[ v\\colon (t, \\xi) \\mapsto v(t,\\xi)\\in \\mathbb R^{3} \\quad\\text{and}\\quad p\\colon (t, \\xi) \\mapsto p(t,\\xi)\\in \\mathbb R. \\] The NSE: \\[\\begin{align*} \\frac{\\partial v}{\\partial t} + (v\\otimes \\nabla_\\xi)v - \\Delta_\\xi v + \\nabla_\\xi p &amp;= 0, \\\\ \\nabla_\\xi \\cdot v &amp;= 0, \\end{align*}\\] with \\(\\otimes\\) denoting the outer product and \\(\\nabla_\\xi\\) and \\(\\Delta_\\xi\\) denoting the gradient and the Laplace operator. If we only count the derivatives with respect to time, as postulated in the introduction, the NSE can be seen as an (abstract) DAE. With dynamical systems, we focus on the evolution of time. That’s why the time derivative is relevant for defining DAEs. Automatic Modelling or Engineers vs. Mathematicians If a system, say an engine, consists of many interacting processes, it is convenient and common practice to model the dynamics of each particular process and to couple the subprocesses through interface conditions. This coupling is done through equating quantities so that the overall model will consist of dynamical equations of the subprocesses and algebraic relations at the interfaces – which makes it a DAE. In fact, tools like modelica for automatic modelling of complex processes do exactly this. The approach of automatic modelling is universal and convenient for engineers. However, the resulting model equations will be DAEs which, as we will see, pose particular problems in their analytical and numerical treatment. 1.2 Why are DAEs difficult to treat Firstly, DAEs do not have the smoothing properties of ODEs, where the solution is one degree smoother than the right hand side. Secondly, the algebraic constraints are essential for the validity of the model. Thus, a numerical approximation may render the model infeasible. Non-smooth Solutions Example 1.3 Consider the equation \\[\\begin{align*} \\dot x_1(t) &amp;= x_2(t) \\\\ 0 &amp; = x_2(t) -g(t) \\end{align*}\\] where \\(g\\) can be a nonsmooth function like \\[ g(t) = \\begin{cases} 0, \\quad\\text{if}\\quad t &lt; 1 \\\\ 1, \\quad\\text{if}\\quad t \\geq 1 \\\\ \\end{cases} \\] In this case the solution part \\(x_1=const. + \\int_0^tg(s)ds\\) will be a smooth function and the solution part \\(x_2=g\\) will have jumps. Even worse, the solution of a DAE may depend on derivatives of the right hand sides. This observation indicates that certain difficulties will arise since numerical approximation schemes require smoothness of the solutions differentiation is numerically ill-posed unlike numerical integration Numerical Solution Means Approximation Imagine the equations (1.1) that describe the pendulum are solved approximately. Then, the algebraic constraint will be violated, i.e. the point mass will leave the circle and the obtained numerical solution becomes infeasible. Thus, special care has to be taken of the algebraic constraints when the equations of motions are numerically integrated. "],["basic-definitions-and-notions.html", "2 Basic Definitions and Notions 2.1 Solution Concept 2.2 Initial Conditions and Consistency 2.3 Additional Remarks", " 2 Basic Definitions and Notions In a very general form, a DAE can be written as \\[\\begin{equation} F(t, x(t), \\dot x(t)) = 0 \\tag{2.1} \\end{equation}\\] with \\(F\\colon \\mathbb I \\times D_x \\times D_{\\dot x} \\to \\mathbb R^m\\) and with a time interval \\(\\mathbb I=[t_0,t_e) \\subset \\mathbb R\\) and state spaces \\(D_x\\), \\(D_{\\dot x} \\subset \\mathbb R^{n}\\) and the task to find a function \\[\\begin{equation*} x \\colon \\mathbb I \\to \\mathbb R^{n} \\end{equation*}\\] with time derivative \\(\\dot x \\colon \\mathbb I \\to \\mathbb R^{n}\\) such that (2.1) is fulfilled for all \\(t\\in I\\). A dynamical process that evolves in time needs an initial state. Thus, one can expect a unique solution to the DAEs only if an initial value is prescribed \\[\\begin{equation} x(t_0) = x_0 \\in \\mathbb R^{n}. \\tag{2.2} \\end{equation}\\] The form of \\(F(t, x(t), \\dot x(t))\\) is a very formal way to write down a system of differential and algebraic equations. X: Write down the equations of the previous examples in this form – i.e. define suitable functions \\(F\\), \\(x\\), and \\(\\dot x\\). 2.1 Solution Concept In order to talk of solutions, we need to define what we understand as a solution. Definition 2.1 \\(\\quad\\) A function \\(x \\in \\mathcal C^1(\\mathbb I, \\mathbb R^{n})\\) is called a solution to the DAE (2.1), if \\(F(t, x(t), \\dot x(t)) = 0\\) holds for all \\(t\\in \\mathbb I\\). A function \\(x \\in \\mathcal C^1(\\mathbb I, \\mathbb R^{n})\\) is called a solution to the initial value problem (2.1) and (2.2), if, furthermore, \\(x(t_0)= x_0\\) holds. An initial condition (2.2) is called consistent for the DAE (2.1), if there exists at least one solution as defined in 2. Some remarks The requirement that \\(x \\in \\mathcal C^1\\) could be relaxed. Compare Example 1.3, where certain components of the solution where smoother than others. Consistency of initial values is a major issue in the treatment of DAEs. See the pendulum… 2.2 Initial Conditions and Consistency We consider again the equations of motions of the pendulum (Example 1.1) \\[\\begin{align*} \\dot x(t) &amp;= u(t) \\\\ \\dot y(t) &amp;= v(t) \\\\ m \\dot u(t) &amp;= - 2 \\lambda(t) (x(t) - c_x) \\\\ m \\dot v(t) &amp;= - 2 \\lambda(t) (y(t) - c_y) - mgy(t) \\end{align*}\\] with the constraint \\[\\begin{equation} 0=(x(t) - c_x)^2 + (y(t) - c_y)^2 - l^2. \\tag{2.3} \\end{equation}\\] To use this model to predict the time evolution of the system, a starting point needs to be known, say for \\(t=0\\). This means initial positions and initial velocities: \\[ \\begin{bmatrix} x(0) \\\\ y(0) \\end{bmatrix} = \\begin{bmatrix} x_0 \\\\ y_0 \\end{bmatrix} \\quad\\text{and}\\quad \\begin{bmatrix} u(0) \\\\ v(0) \\end{bmatrix} = \\begin{bmatrix} u_0 \\\\ v_0 \\end{bmatrix}. \\] The constraint (2.3) needs to be fulfilled at all times and also at \\(t=0\\), which gives the constraint for the initial positions: \\[\\begin{equation*} (x_0 - c_x)^2 + (y_0 - c_y)^2 - l^2=0. \\end{equation*}\\] Moreover, if a constraint \\(h(x(t), y(t))=0\\) holds for all \\(t\\), then, necessarily, \\(\\frac{d}{dt}h=0\\). For the pendulum this means that \\[\\begin{equation} 2(x(t) - c_x)u(t) + 2(y(t) - c_y)v(t) = 0 \\tag{2.4} \\end{equation}\\] must hold for all \\(t\\) and in particular at \\(t=0\\) which gives constraints on the initial velocities \\(u_0\\) and \\(v_0\\): \\[\\begin{equation*} 2(x_0 - c_x)u_0 + 2(y_0 - c_y)v_0 = 0. \\end{equation*}\\] Some remarks on consistency, constraints, and derivations: The so-called consistency conditions on \\((x_0, y_0, u_0, v_0)\\) have the physical interpretation that the initial positions lie on the prescribed circle and that the velocities are tangent to this circle. One can show that the variable \\(\\lambda\\) is completely defined in terms of \\(x\\) and \\(y\\) and their derivatives. Thus, in the formulation (1.1), both in the analysis and in the numerical treatment, there is no need for an initial value for \\(\\lambda\\). However, as we will see, DAEs can be reformulated as ODEs through differentiation and substitutions. In such an ODE formulation, a necessary initial condition for \\(\\lambda\\) will have to fulfill similar consistency conditions as \\((x_0, y_0, u_0, v_0)\\). Condition (2.4) is an example for a hidden-constraint – an algebraic constraint to the system that is not explicit in the original formulation. In theory, condition (2.3) can be replaced by (2.4). Moreover, through differentiation and elimination of constraints, a DAE can be brought into the form of an ODE: in the case of the circuit of Example 1.2 one only needs to replace the constraints by their derivatives: \\[\\begin{equation} \\begin{split} C(\\dot x_3 - \\dot x_2) &amp;= - \\frac{x_1 - x_2}{R} \\\\ \\dot x_1 - \\dot x_3 &amp;= \\dot U \\\\ \\dot x_3 &amp;= 0. \\end{split} \\tag{2.5} \\end{equation}\\] Note that (2.5) can be written as \\(B\\dot x = Ax + f\\) with an invertible matrix \\(B\\) and, thus, is an ODE. For an ODE there is no constraint on the initial values. However, a solution to (2.5) only solves the original DAE (1.2), if the initial values are consistent with the DAE. In this case, this means \\(x_3(t_0)=0\\) and \\(x_1(t_0) - x_3(t_0) = U(t_0)\\). 2.3 Additional Remarks It just took a single derivation to turn the circuit model into an ODE (2.5). For the pendulum this wouldn’t be that easy. The extend of how much algebraic and differential parts are intertwined is measured by indices which is the classifier for DAEs. There are many indices. We will learn about some of the concepts. But first we will introduce some more theory. A low index means that differential and algebraic parts are relatively well separated. (The circuit example is of index 1). A high index means that the structure is more involved. (The pendulum is of index 3). "],["linear-daes-with-constant-coefficients.html", "3 Linear DAEs with Constant Coefficients 3.1 Basic Notions and Definitions 3.2 Regularity and Solvability 3.3 We are here", " 3 Linear DAEs with Constant Coefficients 3.1 Basic Notions and Definitions Consider the DAE in the form \\[\\begin{equation} E \\dot x (t) = Ax(t) + f(t), \\tag{3.1} \\end{equation}\\] where \\(E\\), \\(A\\in \\mathbb R^{m,n}\\) and \\(f\\in \\mathcal C(\\mathbb I, \\mathbb R^m)\\) with, possibly, an initial condition \\[\\begin{equation} x(t_0) = x_0 \\in \\mathbb R^{n}. \\tag{3.2} \\end{equation}\\] For utmost generality, we consider the case that \\(m\\neq n\\), i.e. the number of equations does not meet the number of unknowns, but we will turn to the square case of \\(m=n\\) soon. 3.1.1 Scalings and State Transformations One can confirm that if \\(x\\) is a solution to (3.1) and \\(P\\in \\mathbb R^{n,n}\\) is invertible, then \\(x\\) is a solution to \\[\\begin{equation*} PE \\dot x (t) = PAx(t) + Pf(t). \\end{equation*}\\] This is a scaling of the equations. Similarly, if \\(Q\\in \\mathbb R^{n,n}\\) is invertible, then \\(\\tilde x := Q^{-1}x\\) solves \\[\\begin{equation*} E Q \\dot {\\tilde x} (t) = AQ \\tilde x(t) + f(t). \\end{equation*}\\] This is a state transformation of the system. Thus, when talking of solvability of (3.1), one may equivalently consider any regular \\(Q\\in \\mathbb R^{m,m}\\), \\(P\\in \\mathbb R^{m,m}\\) and the scaled and transformed system \\[\\begin{equation} \\tilde E \\dot{\\tilde x}(t) = \\tilde A \\tilde x (t) + \\tilde f(t), \\quad \\tilde x(0) = Q^{-1}x_0, \\end{equation}\\] where \\(\\tilde E = PEQ\\), \\(\\tilde A = PAQ\\), \\(\\tilde f = Pf\\), and \\(x=Q\\tilde x\\). To characterize all scalings and state transformations, we define these operations as relations of matrix pairs: 3.1.2 Strong Equivalence and Canonical Forms Definition 3.1 Two pairs of matrices \\((E_1, A_1)\\) and \\((E_2, A_2)\\), with \\(E_1\\), \\(A_1\\), \\(E_2\\), \\(A_2 \\in \\mathbb R^{m,n}\\), are called strongly equivalent, if there exist regular matrices \\(P\\in \\mathbb R^{m,m}\\), \\(Q\\in \\mathbb R^{n,n}\\) such that \\[\\begin{equation*} E_2 = PE_1Q, \\quad A_2 = PA_1Q. \\end{equation*}\\] In this case, we write \\[(E_1, A_1) \\sim (E_2, A_2).\\] Lemma 3.1 The relation \\(\\sim\\) defined in Definition 3.1 defines an equivalence relation1. Proof. Exercise. For a given equivalence relation on a set, one can define equivalence classes by considering all members that are equivalent to each other as basically the same. And for each class one may choose a representative, preferably in canonical form, i.e. a form that, e.g., comes with an simple or characteristic representation and that allows for easy determination or analysis of quantities of interest. There can be infinitely many canonical forms. For our purposes and for the strong equivalence of matrix pairs, we will use the Kronecker Canonical Form. Theorem 3.1 Let \\(E\\), \\(A \\in \\mathbb C^{m,n}\\). Then there exist nonsingular matrices \\(P\\in \\mathbb C^{m,m}\\), \\(Q\\in \\mathbb C^{n,n}\\) such that for all \\(\\lambda \\in \\mathbb C\\) \\[\\begin{equation*} P(\\lambda E -A)Q = \\begin{bmatrix} \\mathcal L_{\\epsilon_1} \\\\ &amp; \\ddots \\\\ &amp;&amp; \\mathcal L_{\\epsilon_p} \\\\ &amp;&amp;&amp; \\mathcal M_{\\eta_1} \\\\ &amp;&amp;&amp;&amp; \\ddots \\\\ &amp;&amp;&amp;&amp;&amp; \\mathcal M_{\\eta_q} \\\\ &amp;&amp;&amp;&amp;&amp;&amp; \\mathcal J_{\\rho_1} \\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ddots \\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\mathcal J_{\\rho_r} \\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\mathcal N_{\\sigma_1} \\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\ddots \\\\ &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; \\mathcal N_{\\sigma_s} \\end{bmatrix} \\end{equation*}\\] Where the block entries are as follows: Every entry \\(\\mathcal L_{\\epsilon_j}\\) is bidiagonal of size \\(\\epsilon_j\\times (\\epsilon_j +1)\\), \\(\\epsilon_j \\in \\mathbb N \\cup \\{0\\}\\) of the form \\[\\begin{equation*} \\lambda \\begin{bmatrix} 0 &amp; 1 \\\\ &amp; \\ddots &amp; \\ddots \\\\ &amp;&amp; 0 &amp; 1 \\end{bmatrix} - \\begin{bmatrix} 1 &amp; 0 \\\\ &amp; \\ddots &amp; \\ddots \\\\ &amp;&amp; 1 &amp; 0 \\end{bmatrix} \\end{equation*}\\] Every entry \\(\\mathcal M_{\\eta_j}\\) is bidiagonal of size \\((\\eta_j+1)\\times \\eta_j\\), \\(\\eta_j \\in \\mathbb N \\cup \\{0\\}\\) of the form \\[\\begin{equation*} \\lambda \\begin{bmatrix} 1 \\\\ 0 &amp; \\ddots \\\\ &amp; \\ddots &amp; 1 \\\\ &amp;&amp; 0 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 1 &amp; \\ddots \\\\ &amp; \\ddots &amp; 0 \\\\ &amp;&amp; 1 \\end{bmatrix} \\end{equation*}\\] Every entry \\(\\mathcal J_{\\rho_j}\\) is a Jordan block of size \\(\\rho_j\\times \\rho_j\\), \\(\\rho_j \\in \\mathbb N \\setminus \\{0\\}\\), \\(\\lambda_j \\in \\mathbb C\\) of the form \\[\\begin{equation*} \\lambda \\begin{bmatrix} 1 \\\\ &amp; \\ddots \\\\ &amp;&amp;\\ddots \\\\ &amp;&amp;&amp; 1 \\end{bmatrix} - \\begin{bmatrix} \\lambda_j &amp; 1 \\\\ &amp; \\ddots &amp; \\ddots \\\\ &amp;&amp;&amp; 1 \\\\ &amp;&amp;&amp; \\lambda_j \\end{bmatrix} \\end{equation*}\\] Every entry \\(\\mathcal N_{\\sigma_j}\\) is a nilpotent block of size \\(\\sigma_j\\times \\sigma_j\\), \\(\\sigma_j \\in \\mathbb N \\setminus \\{0\\}\\), of the form \\[\\begin{equation*} \\lambda \\begin{bmatrix} 0 &amp;1\\\\ &amp; \\ddots &amp; \\ddots \\\\ &amp;&amp;&amp; 1 \\\\ &amp;&amp;&amp; 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ &amp; \\ddots \\\\ &amp;&amp;&amp; \\\\ &amp;&amp;&amp;&amp; 1 \\end{bmatrix} \\end{equation*}\\] The Kronecker Canonical Form is uniquely defined up to permutations of the blocks. Proof. Very technical. Can be found, e.g., in the book: Gantmacher (1959) The Theory of Matrices II. Algorithm for computations exist2 but the computation is notoriously ill posed. 3.2 Regularity and Solvability In what follows we will consider the regular case with, among others, \\(E\\) and \\(A\\) as square matrices. Like for solving general equation systems, one can expect well posedness for the case that the numbers of equations equals the number of unknowns. Here squareness of the system is a necessary condition. For sufficiency one further needs that there are no redundant equations or incompatible equations. This is encoded in the regularity. Definition 3.2 Let \\(E\\), \\(A \\in \\mathbb C^{n,n}\\). The matrix pair \\((E, A)\\) is called regular, if the characteristic polynomial \\(p\\), defined via \\[ p(\\lambda) = \\det (\\lambda E- A), \\] is not identically zero. If such a matrix pair is not regular, it is called singular. Is not identically zero means that there exists a \\(\\lambda_0\\) such that \\(p(\\lambda_0)=\\det (\\lambda_0 E - A) \\neq 0\\), i.e. \\(\\lambda_0 E - A\\) is invertible. Since a characteristic polynomial has but a finite numbers of roots (unless it is the zero polynomial), is not identically zero means that \\(p(\\lambda) \\neq 0\\) for all but a few \\(\\lambda\\). Next we show, that regularity is invariant under strong equivalence. This is needed, since we will translate regularity of \\((E,A)\\) to regularity of the associated DAE and we need to ensure that regular scalings and state transformations do not affect regularity. Lemma 3.2 Let \\(E\\), \\(A \\in \\mathbb C^{n,n}\\). If \\((E, A)\\) is strongly equivalent to a regular matrix pair, then \\((E, A)\\) is regular. Proof. Let \\(E_1\\), \\(A_1 \\in \\mathbb C^{n, n}\\) be similar to \\((E, A)\\). By definition, there exist invertible \\(P\\), \\(Q\\in \\mathbb C^{n,n}\\) such that \\(\\lambda E - A = P(\\lambda E_1 -A_1)Q\\) for all \\(\\lambda\\). Thus, \\[ \\det (\\lambda E - A) = \\det P \\det (\\lambda E_1 - A_1) \\det Q \\] is not identically zero, since \\(\\det Q\\) and \\(\\det P\\) are not zero and \\(\\det (\\lambda E_1 - A_1)\\) is not the zero polynomial. With that we can derive a canonical form for strongly equivalent matrix pairs. Theorem 3.2 Let \\(E\\), \\(A \\in \\mathbb C^{n,n}\\) and \\((E,A)\\) be regular. Then \\[\\begin{equation} (E, A) \\sim \\left ( \\begin{bmatrix} I_{n_1} \\\\ &amp; N \\end{bmatrix} , \\begin{bmatrix} J \\\\ &amp; I_{n_2} \\end{bmatrix} \\right ), \\tag{3.3} \\end{equation}\\] where \\(n_1\\), \\(n_2 \\in \\mathbb N\\) such that \\(n=n_1+n_2\\), where \\(I_{n_1}\\) and \\(I_{n_2}\\) denote the identity matrices of size \\(n_1\\times n_1\\) and \\(n_2\\times n_2\\), respectively, where \\(J\\in \\mathbb C^{n_1,n_1}\\) is in Jordan canonical form, and where \\(N \\in \\mathbb C^{n_2,n_2}\\) is a nilpotent matrix. Moreover, it is allowed that the one or the other block is not present, i.e., \\(n_1\\) or \\(n_2\\) can be zero. Proof. To be provided. Recall that the Jordan canonical form can be achieved for any square matrix \\(M\\in \\mathbb C\\) by a similarity transformation. Definition 3.3 (Nilpotent Matrix) A matrix \\(M\\in \\mathbb C^{n,n}\\) is called nilpotent, if there is an integer \\(k\\) such that \\(M^k=0\\). The smallest such integer index, i.e. that \\(\\nu \\in \\mathbb N\\) such that \\(N^\\nu=0\\) whereas \\(N^{\\nu-1} \\neq 0\\) is called the index of nilpotency of \\(M\\). With the convention that \\(\\mathbf 0^0=I\\), the zero matrix \\(\\mathbf 0 \\in \\mathbb R^{n,n}\\) is of (nilpotency) index 1. The relation of solvability and regularity of DAEs becomes evident in the canonical form of Theorem 3.2. In fact, it states that through regular scalings and state transforms, any DAE with \\[ E\\dot x(t) = A x(t) + f(t) \\] with \\((E,A)\\) regular can be transformed and split into \\[\\begin{equation} \\dot x_1(t) = x_1(t) + f_1(t) \\tag{3.4} \\end{equation}\\] and \\[\\begin{equation} N \\dot x_2(t) = x_2(t) + f_2(t) \\tag{3.5}, \\end{equation}\\] i.e into an ODE (3.4) that already is in Jordan canonical form and a separated(!) DAE (3.5) of a particular type. Since linear ODEs always have a unique solution for any initial value, solvability of a general linear DAE with constant, regular coefficients will be completely defined by solvability of the special DAE part (3.5). In what follows we will consider the special DAE \\[\\begin{equation} N \\dot x(t) = x(t) + f(t) \\tag{3.6} \\end{equation}\\] with \\(N \\in \\mathbb R^{n,n}\\) nilpotent with \\(\\nu\\) being the index of nilpotency. For this DAE there is an explicit solution formula: Lemma 3.3 Consider (3.6). If \\(f \\in \\mathcal C^\\nu(\\mathcal I, \\mathbb R^{n})\\), \\(n\\geq 1\\), where \\(\\nu\\) is the index of nilpotency of \\(N\\), then (3.6) has a unique solution given as \\[\\begin{equation} x(t) = - \\sum_i^{\\nu-1}N^if^{(i)}(t), \\tag{3.7} \\end{equation}\\] where \\(f^{(i)}\\) denotes the \\(i\\)-th derivative of \\(f\\). Proof. There are a few ways to prove the explicit form (3.7) Bring \\(N\\) into Jordan canonical form and prove the formula for the Jordan blocks of arbitrary size by induction. Write (3.6) as \\[(N\\frac{d}{dt}-I)x = f\\] and show that3 \\[(N\\frac{d}{dt}-I)^{-1} = - \\sum_i^{\\nu-1}N^i\\frac{d^i}{dt^i}\\]. We take the direct approach as it can be found in the book by Dai4: First, we observe that \\[ x = N\\dot x - f, \\] that (having multiplied by \\(N\\) and differentiated once) \\[ N\\dot x = N^2 \\ddot x - N \\dot f, \\] and that (having muliplied \\(k\\)-times by \\(N\\) and differentiated \\(k\\)-times) \\[ N^k\\dot x^{(k)} = N^{k+1} \\dot x^{(k+1)} - N^k \\dot f^{(k)}. \\] If one successively replaces \\(N^kx^{(k)}=N^{k+1}x^{(k+1)}-N^kf^{(k)}\\), \\(k=1,2,...,\\nu-1\\) in \\[ x=N\\dot x -f = N^2\\ddot x - N\\dot f -f = \\dots = N^\\nu x^{(\\nu)} - \\sum_i^{\\nu-1} N^if^{(i)}, \\] with \\(N^\\nu=0\\), one arrives at formula (3.7). Since this construction holds for any solution, uniqueness is guaranteed too. We make three important observations here. The solution \\(x\\) to (3.6) is uniquely defined without specifying a value at \\(t_0\\). Vice versa, an initial value \\(x_0\\) is consistent if, and only if, \\[ x_0 = - \\sum_i^{\\nu-1}N^if^{(i)}(t_0) \\] The definition of the solution \\(x\\) requires \\(f\\) to be \\(\\nu-1\\)-times differentiable. In order to be a solution according to Definition 2.1, the function \\(x\\) itself has to be differentiable too. Hence the requirement \\(f \\in \\mathcal C^\\nu(\\mathcal I, \\mathbb R^{n})\\). The index of nilpotency of \\(N\\) defines the necessary smoothness of the right hand side. The index of nilpotency in the \\(N\\) part of the Weierstrass canonical form is characteristic for a matrix pair and defines an index of the associated DAE. Definition 3.4 Consider a regular matrix pair \\((E, A)\\) and its Weierstrass canonical form, i.e. \\[ (E, A) \\sim \\left ( \\begin{bmatrix} I_{n_1} \\\\ &amp; N \\end{bmatrix} , \\begin{bmatrix} J \\\\ &amp; I_{n_2} \\end{bmatrix} \\right ). \\] The index \\(\\nu\\) of nilpotency of \\(N\\) is called the index of the matrix pair \\((E, A)\\) and we write \\(\\nu = \\operatorname{ind}(E, A)\\). If \\(N\\) is not present, then we set \\(\\operatorname{ind}(E,A)=0\\). Furthermore, for a nilpotent matrix \\(N\\), we will occasionally use the notion \\(\\nu=\\operatorname{ind}(N,I)\\) to refer to its index of nilpotency. X: Is this OK, i.e. consistent? To be on the safe side and to learn how to handle block structured matrices in the analysis, we confirm that this definition of the index is well-posed, i.e. for any regular matrix \\((E,A)\\) there is a unique index \\(\\nu\\). If one considers the Weierstrass canonical form as a special case of the Kronecker canonical form, then the statement that the canonical form is well-posed up to permutations in the order of the blocks already implies that the index is well defined (since it only depends on the size of the largest nilpotent block but not in which order it appears). Lemma 3.4 Suppose that the regular matrix pair \\((E, A)\\), \\(E\\), \\(A\\in \\mathbb R^{n,n}\\) has the two canonical forms \\[ (E, A) \\sim \\left ( \\begin{bmatrix} I_{d_1} \\\\ &amp; N_1 \\end{bmatrix} , \\begin{bmatrix} J_1 \\\\ &amp; I_{n-d_1} \\end{bmatrix} \\right ) \\sim \\left ( \\begin{bmatrix} I_{d_2} \\\\ &amp; N_2 \\end{bmatrix} , \\begin{bmatrix} J_2 \\\\ &amp; I_{n-d_2} \\end{bmatrix} \\right ), \\] where \\(d_1\\), \\(d_2\\) are the size of the Jordan blocks \\(J_1\\), \\(J_2\\), respectively. Then \\(d_1=d_2=:d\\) and the indices of nilpotency of the nilpotent blocks coincide, i.e. \\(\\operatorname{ind}(N_1, I_{n-d}) = \\operatorname{ind}(N_2, I_{n-d})\\). Proof. to be provided. 3.3 We are here We can now summarize all results and considerations in a theorem. Theorem 3.3 Consider the DAE (3.1) with initial condition (3.2), \\[ E \\dot x (t) = Ax(t) + f(t), \\quad x(t_0) = x_0 \\in \\mathbb R^{n}. \\] Let the pair \\((E, A)\\) be regular and consider the strongly equivalent DAE \\[ \\tilde E \\dot {\\tilde x} (t) = \\tilde A\\tilde x(t) + \\tilde f(t), \\quad \\tilde x(t_0) = \\tilde x_0 \\in \\mathbb R^{n}. \\] with \\((\\tilde E,\\tilde A)\\) in Weierstrass canonical form, i.e. \\[ \\tilde E = \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp;N \\end{bmatrix}, \\quad \\tilde A = \\begin{bmatrix} J &amp; 0 \\\\ 0 &amp;I \\end{bmatrix}, \\] and consider the conforming splitting of the transformed variables \\[ \\tilde x = \\begin{bmatrix} \\tilde x_1 \\\\ \\tilde x_2 \\end{bmatrix}, \\quad \\tilde x_0 = \\begin{bmatrix} \\tilde x_{0,1} \\\\ \\tilde x_{0,2} \\end{bmatrix}. \\] Furthermore, let \\(\\nu\\) be the index of the matrix pair \\((E,A)\\). If \\(f\\in \\mathcal C^\\nu(\\mathcal I,\\mathbb C^{n})\\), then The differential algebraic equation (3.1) is solvable. The initial condition \\(x_0\\) in (3.2) is consistent if, and only if, for the transformed initial condition \\(\\tilde x_0\\) it holds that \\[ \\tilde x_{2,0} = - \\sum_i^{\\nu-1}N^i\\tilde f^{(i)}(t_0) \\] In particular, a consistent initial condition to (3.1) exists. Every initial value problem (3.1)–(3.2) with a consistent initial condition is uniquely solvable. Proof. A summary of the preceding results. From theorem 3.3 it follows that regularity of the matrix pairs \\((E, A)\\) implies the existence of a unique solution to the DAE (3.1) with an initial condition (3.2) provided that the initial condition is consistent. The negation of this statement is a bit diffuse because there are several things that can go wrong if the DAE is not regular. Depending on the irregularity there might be infinite many solutions to the initial value problem or no solutions at all to the DAE (even without the initial condition). Equivalence relation – RST. Reflexive: \\(A\\sim A\\). Symmetric: \\(A\\sim B\\), then \\(B\\sim A\\). Transitive: \\(A\\sim B\\) and \\(B\\sim C\\), then \\(A\\sim C\\).↩ Paul v. Dooren The Computation of Kronecker’s Canonical Form of a Singular Pencil↩ See the proof of Lemma 2.8 in Kunkel/Mehrmann↩ Dai (1989): Singular Control Systems↩ "],["linear-daes-with-time-varying-coefficients.html", "4 Linear DAEs with Time-varying Coefficients", " 4 Linear DAEs with Time-varying Coefficients Theorem 4.1 Let \\(E, A \\in \\mathbb C^{m,n}\\) and let \\[\\begin{equation} T,~Z,~T&#39;,~V \\tag{4.1} \\end{equation}\\] be Matrix as the basis of \\(T\\) \\(\\operatorname{kernel}E\\) \\(Z\\) \\(\\operatorname{corange}E = \\operatorname{kernel}E^H\\) \\(T&#39;\\) \\(\\operatorname{cokernel}E = \\operatorname{range}E^H\\) \\(V\\) \\(\\operatorname{corange}(Z^HAT)\\) then the quantities \\[\\begin{equation} r,~a,~s,~d,~u,~v \\tag{4.2} \\end{equation}\\] defined as Quantity Definition Name \\(r\\) \\(\\operatorname{rank}E\\) rank \\(a\\) \\(\\operatorname{rank}(Z^HAT)\\) algebraic part \\(s\\) \\(\\operatorname{rank}(V^HZ^HAT&#39;)\\) strangeness \\(d\\) \\(r-s\\) differential part \\(u\\) \\(n-r-a\\) undetermined variables \\(v\\) \\(m-r-a-s\\) vanishing equations are invariant under local equivalence transformations and \\((E, A)\\) is locally equivalent to the canonical form \\[\\begin{equation} \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\right), \\tag{4.3} \\end{equation}\\] where all diagonal blocks are square, except maybe the last one. Some remarks on the spaces and how the names are derived for the case \\(E\\dot x = Ax +f\\) with constant coefficients. The ideas are readily transferred to the case with time-varying coefficients. Let \\[x(t) = Ty(t) + T&#39;y&#39;(t),\\] where \\(y\\) denotes the components of \\(x\\) that evolve in the range of \\(T\\) and \\(y&#39;\\) the respective complement. (Since \\([T|T&#39;]\\) is a basis of \\(\\mathbb C^{n}\\), there exist such \\(y\\) and \\(y&#39;\\) that uniquely define \\(x\\) and vice versa). With \\(T\\) spanning \\(\\ker E\\) we find that \\[E \\dot x(t) = ET\\dot y(t) + ET&#39;\\dot y&#39;(t)\\] so that the DAE basically reads \\[ET&#39;\\dot y&#39;(t) = ATy(t) + AT&#39;y&#39;(t)+f,\\] i.e. the components of \\(x\\) defined through \\(y\\) are, effectively, not differentiated. With \\(Z\\) containing exactly those \\(v\\), for which \\(v^HE=0\\), it follows that \\[Z^HET&#39;\\dot y&#39;(t) = 0 = Z^HATy(t) + Z^HAT&#39;y&#39;(t)+Z^Hf,\\] or \\[Z^HATy(t) = -Z^HAT&#39;y&#39;(t)-Z^Hf,\\] so that \\(\\operatorname{rank}Z^HAT\\) indeed describes the number of purely algebraic equations and variables in the sense that it defines parts of \\(y\\) (which is never going to be differentiated) in terms of algebraic relations (no time derivatives are involved). With the same arguments and with \\(V=\\operatorname{corange}Z^HAT\\), it follows that \\[V^HZ^HAT&#39;y&#39;(t) = -V^HZ^HATy(t) -V^HZ^Hf=-V^HZ^Hf,\\] is the part of \\(E\\dot x = Ax + f\\) in which those components \\(y&#39;\\) that are also differentiated are algebraically equated to a right-hand side. This is the strangeness (rather in the sense of skewness) of DAEs that variables can be both differential and algebraic. Accordingly, \\(\\operatorname{rank}V^HZ^HAT&#39;\\) describes the size of the skewness component. Outlook: If there is no strangeness, the DAE is called strangeness-free. Strangeness can be eliminated through iterated differentiation and substitution. The needed number of such iterations (that is independent of the the size \\(s\\) of the strange block here) will define the strangeness index. Example 4.1 With basic scalings and state transforms, one finds for the coefficients of Example 1.2 that: \\[ (E, A) \\backsim \\left( \\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; I_1 \\end{bmatrix} \\right). \\] We compute the subspaces as defined in (4.1): Matrix as the basis of/computed as \\(T=\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}\\) \\(\\operatorname{kernel}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\) \\(Z=\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}\\) \\(\\operatorname{corange}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}=\\operatorname{kernel}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}^H\\) \\(T&#39;=\\begin{bmatrix} I_2 \\\\ 0 \\end{bmatrix}\\) \\(\\operatorname{cokernel}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}=\\operatorname{range}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}^H\\) \\(Z^HAT=I_1\\) \\(\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}^H\\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; I_1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}\\) \\(V=0\\) \\(\\operatorname{corange}(Z^HAT) = \\operatorname{kernel}I_1^H\\phantom{\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}}\\) \\(Z^HAT&#39;=0_{2\\times 1}\\) \\(\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}^H\\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; I_1 \\end{bmatrix}\\begin{bmatrix} I_2 \\\\ 0 \\end{bmatrix}\\) and derive the quantities as defined in (4.2): Name Value Derived from rank \\(r=2\\) \\(\\operatorname{rank}E = \\operatorname{rank}\\begin{bmatrix} I_2 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\) algebraic part \\(a=1\\) \\(\\operatorname{rank}Z^HAT = \\operatorname{rank}I_1\\) strangeness \\(s=0\\) \\(\\operatorname{rank}V^HZ^HAT&#39; = \\operatorname{rank}0_{2\\times 1}\\) differential part \\(d=2\\) \\(d=r-s=2-0\\) undetermined variables \\(u=0\\) \\(u=n-r-a=3-2-1\\) vanishing equations \\(v=0\\) \\(v=m-r-a-s=3-2-1-0\\) Example 4.2 With more involved scalings and state transforms, one finds for the coefficients of the linearized and spatially discretized Navier-Stokes equations (see Exercise I) that: \\[ (\\mathcal E, \\mathcal A) = \\left( \\begin{bmatrix} M &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} A &amp; B^H \\\\ B &amp; 0 \\end{bmatrix} \\right) \\backsim \\left( \\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix} , \\begin{bmatrix} A_{11} &amp; A_{12} &amp; I_{n_1} \\\\ A_{21} &amp; A_{22} &amp; 0 \\\\ I_{n_1} &amp; 0 &amp; 0\\end{bmatrix} \\right). \\] We compute the subspaces as defined in (4.1): Matrix as the basis of/computed as \\(T=\\begin{bmatrix} 0 \\\\ 0 \\\\I_{n_1} \\end{bmatrix}\\) \\(\\operatorname{kernel}\\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix}\\) \\(Z=\\begin{bmatrix} 0 \\\\ 0 \\\\I_{n_1} \\end{bmatrix}\\) \\(\\operatorname{corange}\\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix}\\) \\(T&#39;=\\begin{bmatrix} I_{n_1} &amp; 0 \\\\ 0 &amp; I_{n_2} \\\\ 0 &amp; 0 \\end{bmatrix}\\) \\(\\operatorname{cokernel}\\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix}\\) \\(Z^HAT=0_{n_1}\\) \\(\\begin{bmatrix} 0 \\\\ 0 \\\\I_{n_1} \\end{bmatrix}^H\\begin{bmatrix} A_{11} &amp; A_{12} &amp; I_{n_1} \\\\ A_{21} &amp; A_{22} &amp; 0 \\\\ I_{n_1} &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\\\I_{n_1} \\end{bmatrix}\\) \\(V=I_{n_1}\\) \\(\\operatorname{corange}(Z^HAT) = \\operatorname{kernel}0_{n_1}^H\\phantom{\\begin{bmatrix} 0 \\\\ I_1 \\end{bmatrix}}\\) \\(Z^HAT&#39;=\\begin{bmatrix} I_{n_1} &amp; 0_{n_1\\times n_2}\\end{bmatrix}\\) \\(\\begin{bmatrix} 0 \\\\ 0 \\\\I_{n_1} \\end{bmatrix}^H\\begin{bmatrix} A_{11} &amp; A_{12} &amp; I_{n_1} \\\\ A_{21} &amp; A_{22} &amp; 0 \\\\ I_{n_1} &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix} I_{n_1} &amp; 0 \\\\ 0 &amp; I_{n_2} \\\\ 0 &amp; 0 \\end{bmatrix}\\) and derive the quantities as defined in (4.2): Name Value Derived from rank \\(r=n_1+n_2\\) \\(\\operatorname{rank}E = \\operatorname{rank}\\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix}\\) algebraic part \\(a=0\\) \\(\\operatorname{rank}Z^HAT = \\operatorname{rank}0_{n_1}\\) strangeness \\(s=n_1\\) \\(\\operatorname{rank}V^HZ^HAT&#39; = \\operatorname{rank}\\begin{bmatrix} I_{n_1} &amp; 0_{n_1\\times n_2}\\end{bmatrix}\\) differential part \\(d=n_2\\) \\(d=r-s=(n_1 + n_2) - n_1\\) undetermined variables \\(u=n_1\\) \\(u=n-r-a=(n_1+n_2+n_1)-(n_1+n_2)-0\\) vanishing equations \\(v=0\\) \\(v=m-r-a-s=(n_1+n_2+n_1)-(n_1+n_2)-n_1\\) Theorem 4.2 (see Kunkel/Mehrmann, Thm. 3.9) Let \\(E\\in \\mathcal C^l(I, \\mathbb C^{m,n})\\) with \\(\\operatorname{rank}E(t)=r\\) for all \\(t\\in I\\). Then there exist smooth and pointwise unitary (and, thus, nonsingular) matrix functions \\(U\\) and \\(V\\), such that \\[ U^HEV = \\begin{bmatrix} \\Sigma &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\] with pointwise nonsingular \\(\\Sigma \\in \\mathcal C^l(I, \\mathbb C^{r,r})\\). Theorem 4.3 Let \\(E, A \\in \\mathcal C^l(I, \\mathbb C^{m,n})\\) be sufficiently smooth and suppose that \\[\\begin{equation} r(t) = r, \\quad a(t)=a, \\quad s(t)=s \\tag{4.4} \\end{equation}\\] for the local characteristic values of \\((E(t), A(t))\\). Then \\((E, A)\\) is globally equivalent to the canonical form \\[\\begin{equation} \\left( \\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12}&amp; 0 &amp; A_{14} \\\\ 0 &amp; 0 &amp; 0 &amp; A_{24} \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\right ). \\tag{4.5} \\end{equation}\\] All entries are again matrix functions on \\(I\\) and the last block column in both matrix functions of (4.5) has size \\(u=n-s-d-a\\). Proof. In what follows, we will tacitly redefine the block matrix entries that appear after the global equivalence transformations. The first step is the continous SVD of \\(E\\); see Theorem 4.3. \\[\\begin{align*} (E,A) &amp; \\sim \\left(\\begin{bmatrix} \\Sigma &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{bmatrix}\\right) \\\\ %%%%%%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{bmatrix}\\right) \\\\ %%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11} &amp; A_{12}V_1 \\\\ U_1^HA_{21} &amp; U_1^HA_{22}V_1 \\end{bmatrix} - \\begin{bmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; \\dot V_1 \\end{bmatrix} \\right) \\\\ %%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_r &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11} &amp; A_{12} &amp; A_{13}\\\\ A_{21} &amp; I_a &amp; 0 \\\\ A_{31} &amp; 0 &amp; 0 \\end{bmatrix}\\right) \\\\ %%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} V_2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11}V_2 &amp; A_{12} &amp; A_{13}\\\\ A_{21}V_2 &amp; I_a &amp; 0 \\\\ U_2^HA_{31}V_2 &amp; 0 &amp; 0 \\end{bmatrix} - \\begin{bmatrix} \\dot I_r &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\dot V_2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\right)\\\\ %%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} V_{11} &amp; V_{12} &amp; 0 &amp; 0 \\\\ V_{21} &amp; V_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} A_{11} &amp; A_{12} &amp; A_{13} &amp; A_{14} \\\\ A_{21} &amp; A_{22} &amp; A_{23} &amp; A_{24} \\\\ A_{31} &amp; A_{32} &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\right)\\\\ %%%%%%%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12} &amp; A_{13} &amp; A_{14} \\\\ 0 &amp; A_{22} &amp; A_{23} &amp; A_{24} \\\\ 0 &amp; A_{32} &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; -A_{32} &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; I_a \\end{bmatrix} \\right) \\\\ %%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12} &amp; A_{13} &amp; A_{14} \\\\ 0 &amp; A_{22} &amp; A_{23} &amp; A_{24} \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\right)\\\\ %%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12} &amp; 0 &amp; A_{14} \\\\ 0 &amp; A_{22} &amp; 0 &amp; A_{24} \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\right)\\\\ %%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; Q_2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12}Q_2 &amp; 0 &amp; A_{14} \\\\ 0 &amp; A_{22}Q_2-\\dot Q_2 &amp; 0 &amp; A_{24} \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\right) \\\\ %%%%%%%%%%%%%%%%% &amp; \\sim \\left(\\begin{bmatrix} I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I_d &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}, \\begin{bmatrix} 0 &amp; A_{12} &amp; 0 &amp; A_{14} \\\\ 0 &amp; 0 &amp; 0 &amp; A_{24} \\\\ 0 &amp; 0 &amp; I_a &amp; 0 \\\\ I_s &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\right), \\end{align*}\\] where the final equivalence holds, if \\(Q_2\\) is chosen as the (unique and pointwise invertible) solution of the linear matrix valued ODE \\[ \\dot Q_2 = A_{22}(t)Q_2 , \\quad Q_2 (t_0 ) = I_d. \\] Then, \\(A_{22}\\) vanishes because of the special choice of \\(Q_2\\) and \\(E_{22}\\) becomes \\(I_d\\) after scaling the second block line by \\(Q_2^{-1}\\). "],["numerical-approximation-of-daes.html", "5 Numerical Approximation of DAEs", " 5 Numerical Approximation of DAEs In order to analyse the approximation error of the RKM \\((\\mathcal A, \\beta, \\gamma)\\) applied to a regular linear DAE with constant coefficients \\[ E\\dot x = Ax+f(t). \\] Without loss of generality, we can assume that \\((E,A)\\) is in Kronecker Canonical Form \\(\\leftarrow\\) RKM are invariant under equivalence transformation \\((E,A)=(N,I)\\) \\(\\leftarrow\\) the regular part can be treated by ODE theory \\(E=N=N_\\nu\\) consists of a single Jordan block \\(\\leftarrow\\) otherwise consider each Jordan block separately Thus, we can consider the special DAE \\[\\begin{equation} \\begin{bmatrix} 0 &amp; 1 &amp; &amp; &amp; \\\\ &amp; 0 &amp; 1 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; 0 &amp; 1 \\\\ &amp; &amp; &amp; &amp; 0 \\end{bmatrix} \\dot x = x + f(t), \\tag{5.1} \\end{equation}\\] where \\[ x(t) = \\begin{bmatrix} x_1(t) \\\\ x_2(t) \\\\ \\vdots \\\\ x_\\nu(t) \\end{bmatrix} \\quad\\text{and}\\quad f(t) = \\begin{bmatrix} f_1(t) \\\\ f_2(t) \\\\ \\vdots \\\\ f_\\nu(t) \\end{bmatrix} \\] Theorem 5.1 The local error of an RKM with \\(\\mathcal A\\) invertible applied to (5.1) behaves like \\[ x(t_{i+1}) - x_{i+1} = \\mathcal O(h^{\\kappa_\\nu - \\nu + 2} + h^{\\kappa_{\\nu-1} - \\nu + 3} + \\cdots + h^{\\kappa_1 +1}) \\] where \\(\\kappa_j\\) is the maximum number such that Condition range of \\(k\\) a.) \\(\\beta^T\\mathcal A^{-k}e = \\beta^T\\mathcal A^{-j}\\gamma^{j-k} / (j-k)!\\) \\(k=1,2,\\cdots,j-1\\) b.) \\(\\beta^T\\mathcal A^{-j}\\gamma^k = k! / (k-j+1)!\\) \\(k=j,j+1,\\cdots\\) for all \\(k\\leq \\kappa_j\\) and for \\(j=1, \\cdots, \\nu\\). Proof. Since we consider the pure consistency error, we can assume that \\(x_i=x(t_i)\\). With that and with the definition of the RKM, the error is given as \\[ \\tau = x(t_{i+1}) - x_{i+1} = -h\\sum_{j=1}^s \\beta_j \\dot X_{ij} + \\sum_{k\\geq 1} \\frac{h^k}{k!}x^{(k)}(t_i). \\] Because of the special structure of the DAE, we can concentrate on the first error component \\(\\tau_1\\) \\(\\leftarrow\\) the error component \\(\\tau_2\\) is the first component of the problem of index \\(\\nu-1\\). For \\(\\tau_1\\) we have the formula \\[ \\tau_1 = x_1(t_{i+1}) - x_{i+1,1} = h\\beta^T\\sum_{j=1}^\\nu (h\\mathcal A)^{-j}Z_{ij} + \\sum_{k\\geq 1} \\frac{h^k}{k!}x_1^{(k)}(t_i). \\] One may confirm directly, or by means of the solution formula for \\(N\\dot x = x + f\\), that the \\(\\ell\\)-th component of \\(x\\) is defined as \\[ x_\\ell(t) = - \\sum_{j=\\ell}^{\\nu}f_j^{(j-\\ell)}(t). \\] The componentwise Taylor expansion of \\(Z_{i,\\ell}\\) reads \\[\\begin{align*} Z_{i\\ell} &amp;= \\begin{bmatrix} x_{i,\\ell} + f_\\ell(t_i+\\gamma_1 h) \\\\ x_{i,\\ell} + f_\\ell(t_i+\\gamma_2 h) \\\\ \\vdots \\\\ x_{i,\\ell} + f_\\ell(t_i+\\gamma_s h) \\end{bmatrix} = \\begin{bmatrix} x_{i,\\ell} + f_\\ell(t_i) + \\sum_{k\\geq 1}\\frac{h^k}{k!}f_\\ell^{(k)}(t_i)\\gamma_1^k \\\\ x_{i,\\ell} + f_\\ell(t_i) + \\sum_{k\\geq 1}\\frac{h^k}{k!}f_\\ell^{(k)}(t_i)\\gamma_2 ^k \\\\ \\vdots \\\\ x_{i,\\ell} + f_\\ell(t_i) + \\sum_{k\\geq 1}\\frac{h^k}{k!}f_\\ell^{(k)}(t_i)\\gamma_s ^k \\end{bmatrix} \\\\ &amp;= x_{i,\\ell}e+\\sum_{k\\geq 0} \\frac{h^k}{k!}f_\\ell^{(k)}(t_i)\\gamma^k \\end{align*}\\] With that and with \\(x_i=x(t_i)\\), we expand the error \\(\\tau_1\\) as follows: \\[\\begin{align*} \\tau_1 &amp;= \\beta^T\\sum_{j=1}^\\nu (h\\mathcal A)^{-j}Z_{ij} + \\sum_{k\\geq 1} \\frac{h^k}{k!}x_1^{(k)}(t_i)\\\\ &amp;= \\beta^T\\sum_{j=1}^\\nu h^{-j+1}\\mathcal A^{-j}\\bigr[ x_{j}(t_i)e+\\sum_{k\\geq 0} \\frac{h^k}{k!}f_j^{(k)}(t_i)\\gamma^k\\bigr] \\\\&amp;\\quad\\quad\\quad\\quad+ \\sum_{k\\geq 1} \\frac{h^k}{k!}x_1^{(k)}(t_i)\\\\ &amp;= \\beta^T\\sum_{j=1}^\\nu h^{-j+1}\\mathcal A^{-j}\\bigr[ -\\sum_{k=j}^{\\nu}f_k^{(k-j)}(t_i)e+\\sum_{k\\geq 0} \\frac{h^k}{k!}f_j^{(k)}(t_i)\\gamma^k\\bigr] \\\\&amp;\\quad\\quad\\quad\\quad- \\sum_{k\\geq 1} \\frac{h^k}{k!} \\sum_{j=1}^{\\nu}f_j^{(j-1+k)}(t_i)\\\\ &amp;= -\\sum_{j=1}^\\nu \\sum_{k=j}^{\\nu} h^{-j+1}\\beta^T\\mathcal A^{-j} ef_k^{(k-j)}(t_i)+\\sum_{j=1}^\\nu \\sum_{k\\geq 0}\\frac{h^{k-j+1}}{k!}\\beta^T\\mathcal A^{-j} \\gamma^k f_j^{(k)}(t_i) \\\\&amp;\\quad\\quad\\quad\\quad- \\sum_{k\\geq 1}\\sum_{j=1}^{\\nu} \\frac{h^k}{k!} f_j^{(j-1+k)}(t_i), \\end{align*}\\] which, with \\(\\sum_{j=1}^\\nu \\sum_{k=j}^\\nu g(j,k) = \\sum_{k=1}^\\nu \\sum_{j=1}^k g(j,k)= \\sum_{k=1}^\\nu \\sum_{j=1}^k g(k,j)\\), becomes \\[\\begin{align*} \\tau_1 &amp;= \\sum_{j=1}^{\\nu} \\bigl[ -\\sum_{k=1}^j h^{-k+1}\\beta^T\\mathcal A^{-k} ef_k^{(j-k)}(t_i)\\\\&amp;\\quad\\quad\\quad\\quad+\\sum_{k\\geq 0}\\frac{h^{k-j+1}}{k!}\\beta^T\\mathcal A^{-j} \\gamma^k f_j^{(k)}(t_i) \\\\&amp;\\quad\\quad\\quad\\quad- \\sum_{k\\geq 1}\\frac{h^k}{k!} f_j^{(j-1+k)}(t_i) \\bigr] \\\\ &amp;= \\sum_{j=1}^{\\nu} \\bigl[ -\\sum_{k=1}^j h^{-k+1}\\beta^T\\mathcal A^{-k} ef_k^{(j-k)}(t_i)\\\\ &amp;\\quad\\quad\\quad\\quad+\\sum_{k=0}^{j-1}\\frac{h^{k-j+1}}{k!}\\beta^T\\mathcal A^{-j} \\gamma^k f_j^{(k)}(t_i)+\\sum_{k\\geq j}\\frac{h^{k-j+1}}{k!}\\beta^T\\mathcal A^{-j} \\gamma^k f_j^{(k)}(t_i) \\\\&amp;\\quad\\quad\\quad\\quad- \\sum_{k\\geq 1}\\frac{h^k}{k!} f_j^{(j-1+k)}(t_i) \\bigr]. \\end{align*}\\] A shift of indices, \\(\\sum_{k=0}^{j-1}g(k)=\\sum_{k=1}^j g(j-k)\\) and \\(\\sum_{k\\geq 1}g(k)=\\sum_{k\\geq j}g(k-j+1)\\), then gives: \\[\\begin{align*} \\tau_1 &amp;= \\sum_{j=1}^{\\nu} \\bigl[ -\\sum_{k=1}^j h^{-k+1}\\beta^T\\mathcal A^{-k} ef_k^{(j-k)}(t_i)+\\sum_{k=1}^{j}\\frac{h^{-k+1}}{(j-k)!}\\beta^T\\mathcal A^{-j} \\gamma^{j-k} f_j^{(j-k)}(t_i)\\\\ &amp;\\quad\\quad\\quad\\quad+\\sum_{k\\geq j}\\frac{h^{k-j+1}}{k!}\\beta^T\\mathcal A^{-j} \\gamma^k f_j^{(k)}(t_i) - \\sum_{k\\geq j}\\frac{h^{k-j+1}}{(k-j+1)!} f_j^{(k)}(t_i) \\bigr]. \\end{align*}\\] "],["construction-and-analysis-of-rkm-for-nonlinear-daes.html", "6 Construction and Analysis of RKM for nonlinear DAEs 6.1 General RKM for Semi-Explicit Strangeness-free DAEs 6.2 Collocation RKM for Implicit Strangeness-free DAEs", " 6 Construction and Analysis of RKM for nonlinear DAEs Now we consider RKM for nonlinear DAEs. We start with a DAE in semi explicit strangeness-free form and give general results on how to write down a general RKM for it and how to analyse the global error. Then, we consider general strangeness-free nonlinear DAEs and show that a certain class of RKM applies well – namely those that can be constructed by collocation with Lagrange polynomials over the Radau, Lobatto, or Gauss quadrature points. 6.1 General RKM for Semi-Explicit Strangeness-free DAEs A semi explicit strangeness-free DAE is of the form \\[\\begin{align} \\dot x &amp;= f(t, x, y) \\tag{6.1} \\\\ 0 &amp;= g(t, x, y) \\tag{6.2} \\end{align}\\] with the Jacobian of \\(g\\) with respect to \\(y\\), i.e. \\[ \\partial_y\\otimes g(t, x(t), y(t)) =: g_y(t, x(t), y(t)), \\] being invertible for all \\(t\\) along the solution \\((x,y)\\). Some observations: this system is strangeness-free under certain assumptions, any DAE can be brought into this form in the linear case \\(E\\dot z = Az +f\\), with \\(z=(x,y)\\), the assumptions basically mean that \\[ E = \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\quad\\text{and}\\quad A = \\begin{bmatrix} * &amp; * \\\\ * &amp; A_{22} \\end{bmatrix}, \\] with \\(A_{22}(t)\\) invertible for all \\(t\\). The condition \\(g_y\\) invertible means that, locally, one could consider \\[ \\dot x = f(t, x, R(t,x)), \\quad\\text{with $R$ such that}\\quad y=R(t,x). \\] However, this is not practical for numerical purposes. The general strategy to get a suitable formulation of a time discretization of system (6.1)-(6.2) by any RKM is to consider the perturbed version \\[\\begin{align*} \\dot x = f(t, x, y), \\\\ \\varepsilon \\dot y = g(t, x, y), \\end{align*}\\] which is an ODE, formulate the RKM, and then let \\(\\varepsilon \\to 0\\). In the Hairer/Wanner Book, this approach is called *\\(\\varepsilon\\)-embedding. This is, consider Table 6.1: RKM applied to semi-explicit DAEs \\(x_{i+1} = x_i + h\\sum_{j=1}^s\\beta_j \\dot X_{ij}\\), \\(y_{i+1} = y_i + h\\sum_{j=1}^s\\beta_j \\dot Y_{ij}\\), \\(\\dot X_{ij} = f(t_i+\\gamma_jh, X_{ij}, Y_{ij})\\), \\(\\varepsilon \\dot Y_{ij} = g(t_i+\\gamma_j h, X_{ij}, Y_{ij})\\), \\(j=1,2,\\cdots,s, \\quad \\quad (*)\\) \\(X_{ij} = x_i + h\\sum_{\\ell=1}^s\\alpha_{j\\ell}\\dot X_{i\\ell}\\), \\(\\phantom{\\varepsilon}Y_{ij} = y_i + h\\sum_{\\ell=1}^s\\alpha_{j\\ell}\\dot Y_{i\\ell}\\), \\(j=1,2,\\cdots,s,\\) i.e., the RKM applied to an ODE in the variables \\((x,y)\\), and replace \\((*)\\) by \\[ \\dot X_{ij} = f(t_i+\\gamma_jh, X_{ij}, Y_{ij}),\\quad 0 = g(t_i+\\gamma_j h, X_{ij}, Y_{ij}), \\quad j=1,2,\\cdots,s. \\] Theorem 6.1 (Kunkel/Mehrmann Thm. 5.16) Consider a semi-explicit, strangeness-free DAE as in (6.1)-(6.2) with a consistent initial value \\((x_0, y_0)\\). The time-discretization by a RKM, with \\(\\mathcal A\\) invertible and \\(\\rho:=1-\\beta^T\\mathcal A^{-1}e\\), applied as in Table 6.1 with \\(\\varepsilon=0\\), that is convergent of order \\(p\\) for ODEs and fulfills the Butcher condition \\(C(q)\\) with \\(q\\geq p+1\\) leads to an global error that behaves like \\[ \\|\\mathfrak X(t_N) - \\mathfrak X_N\\| = \\mathcal O(h^k), \\] where \\(k=p\\), if \\(\\rho=0\\), \\(k=\\min\\{p, q+1\\}\\), if \\(-1\\leq \\rho &lt; 1\\) \\(k=\\min\\{p, q-1\\}\\), if \\(\\rho =1\\). If \\(|\\rho|&gt;1\\), then the RKM – applied to (6.1)–(6.2) – does not converge. Some words on the conditions on \\(p\\), \\(q\\), and \\(\\rho\\): For stiffly accurate methods, \\(\\beta^T \\mathcal A^{-1}e=1\\) and, thus, \\(\\rho=0\\) \\(\\rightarrow\\) no order reduction for strangeness free or index-1 systems For the implicit midpoint rule also known as the 1-stage Gauss method: \\[ \\begin{array}{c|c} \\frac 12 &amp; \\frac 12 \\\\ \\hline &amp; 1 \\end{array} \\] the convergence order for ODEs is \\(p=2\\) but \\(1-\\beta^T \\mathcal A^{-1}e = 1- 1\\cdot {\\bigl(\\frac 12\\bigr)}^{-1} 1 = -1\\), so that \\(\\min\\{p-1, q\\} = k \\leq 1\\), depending on \\(q\\). in fact \\(k=1\\) as \\[ C(q): \\quad \\sum_{\\ell=1}^s\\alpha_{j\\ell}\\gamma_\\ell^{\\bar k-1}=\\frac{1}{\\bar k} \\gamma_j^{\\bar k}, \\quad {\\bar k}=1,\\cdots,q, \\quad j=1,\\cdots,s \\] in the present case of \\(s=1\\), \\(\\alpha_{11}=\\gamma_1=\\frac 12\\) is fulfilled for \\({\\bar k}=1: \\quad \\frac 12 = \\frac 12\\) it is not relevant here, but for \\({\\bar k}=2:\\quad \\frac 12 \\cdot \\frac 12 \\neq \\frac 12 \\cdot \\frac 14\\) 6.2 Collocation RKM for Implicit Strangeness-free DAEs The general form of a strangeness-free DAE is given as \\[\\begin{align} \\hat F_1(t,x,\\dot x) &amp;= 0 \\tag{6.3}\\\\ \\hat F_2(t,x) &amp;= 0 \\tag{6.4} \\end{align}\\] where the strangeness-free or index-1 assumption is encoded in the existence of implicit functions \\(\\mathcal L\\), \\(\\mathcal R\\) such that, with \\(x=(x_1,x_2)\\), the implicit DAE (6.3)–(6.4) is equivalent to the semi-explicit DAE \\[\\begin{align*} \\dot x_1 &amp;= \\mathcal L(t,x_1,x_2) \\\\ 0 &amp;= \\mathcal R(t,x_1) -x_2 \\end{align*}\\] In what follows we show that a collocation approach coincides with certain RKM discretizations so that the convergence analysis of the RKM can be done via approximation theory. Regression (Collocation): – If one looks for a function \\(x\\colon [0,1] \\to \\mathbb R^{}\\) that fulfills \\(F(x(t))=0\\) for all \\(t\\in[0,1]\\), one may interpolate \\(x\\) by, say, a polynomial \\(x_p(t) = \\sum_{\\ell=0}^kx_\\ell t^\\ell\\) and determine the \\(k+1\\) coefficients \\(x_\\ell\\) via the solution of the system of (nonlinear) equations \\(F(x_p(t_\\ell))=0\\), \\(\\ell=0,1,\\dotsc,k\\), where the \\(t_\\ell\\in[0,1]\\) are the \\(k+1\\) collocation points. Concretely, we parametrize \\(s\\) collocation points via \\[\\begin{equation} 0&lt; \\gamma_1 &lt;\\gamma_2&lt; \\dotsc &lt; \\gamma_s=1 \\tag{6.5} \\end{equation}\\] and define two sets of Lagrange polynomials \\[ L_\\ell(\\xi) = \\prod_{j=0,j\\neq \\ell}^s \\frac{\\xi-\\gamma_j}{\\gamma_\\ell-\\gamma_j} \\quad\\text{and}\\quad \\tilde L_\\ell(\\xi) = \\prod_{m=1,m\\neq \\ell}^s \\frac{\\xi-\\gamma_m}{\\gamma_\\ell-\\gamma_m}, \\] with \\(\\ell\\in\\{0,1,\\dotsc,s\\}\\). Let \\(\\mathbb P_k\\) be the space of polynomials of degree \\(\\leq k-1\\). We define the collocation polynomial \\(x_\\pi \\in \\mathbb P_{s+1}\\) via \\[\\begin{equation} x_\\pi (t) = \\sum_{\\ell=0}^s X_{i\\ell}L_\\ell\\bigl(\\frac{t-t_i}{h}\\bigr) \\tag{6.6} \\end{equation}\\] designed to compute the stage values \\(X_{i\\ell}\\), where \\(X_{i0}=x_i\\) is already given. The stage derivatives are then defined as \\[\\begin{equation} \\dot X_{ij} = \\dot x_\\pi(t_i+\\gamma_jh) = \\frac 1h \\sum_{\\ell=0}^sX_{i\\ell}\\dot L_\\ell(\\gamma_j). \\tag{6.7} \\end{equation}\\] To obtain \\(x_{i+1}=x_\\pi(t_{i+1})=X_{is}\\), we require the polynomial to satisfy the DAE (6.3)–(6.4) at the collocation points \\(t_{ij}=t_i+\\gamma_jh\\), that is \\[\\begin{equation} \\hat F_1(t_i+\\gamma_jh,X_{ij},\\dot X_{ij}) = 0, \\quad \\hat F_2(t_i+\\gamma_jh,X_{ij}) = 0, \\quad j=1,\\dotsc,s. \\phantom{F_1} \\tag{6.8} \\end{equation}\\] Now we show that this collocation defines a RKM discretization of (6.3)–(6.4). Since \\(\\tilde L \\in \\mathbb P_s\\), it holds that \\[ P_\\ell(\\sigma):=\\int_0^\\sigma \\tilde L_\\ell (\\xi)d\\xi \\in \\mathbb P_{s+1} \\] that is, by Lagrange interpolation, it can be written as \\[ P_\\ell(\\sigma) = \\sum_{j=0}^s P_\\ell(\\gamma_j)L_j(\\sigma). \\] If we differentiate \\(P_l\\), we get \\[ \\dot P_\\ell(\\sigma) = \\sum_{j=0}^s P_\\ell(\\gamma_j)\\dot L_j(\\sigma) = \\sum_{j=0}^s \\int_0^{\\gamma_j} \\tilde L_\\ell (\\xi)d\\xi \\dot L_j(\\sigma)=: \\sum_{j=0}^s \\alpha_{j\\ell} \\dot L_j(\\sigma) \\] where define simply define \\[ \\alpha_{j\\ell} = \\int_0^{\\gamma_j} \\tilde L_\\ell (\\xi)d\\xi. \\] Moreover, by definition of \\(P_\\ell\\) (and the fundamental theorem of calculus), it holds that \\[ \\dot P_\\ell(\\sigma) = \\tilde L_\\ell(\\sigma), \\] which gives that \\(\\dot P_\\ell(\\gamma_m) = \\delta_{\\ell m}\\) that is \\[ \\dot P_\\ell(\\gamma_m) = \\sum_{j=1}^s\\alpha_{j\\ell}\\dot L_j(\\gamma_m) = \\begin{cases} 1, &amp;\\quad \\text{if }\\ell =m \\\\ 0, &amp;\\quad \\text{otherwise} \\end{cases}. \\] for \\(\\ell, m=1,\\dotsc,s\\). Accordingly, if we define \\(\\mathcal A := \\bigl[\\alpha_{j\\ell}\\bigr]_{j,\\ell=1,\\dotsc,s} \\in \\mathbb R^{s,s}\\) and \\[ V:=\\bigl[v_{mj}\\bigr]_{m,j=1,\\dotsc,s} = \\bigl[ \\dot L_j(\\gamma_m) \\bigr]_{m,j=1,\\dotsc,s} \\in \\mathbb R^{s,s} , \\] it follows that \\(V=\\mathcal A^{-1}\\). Moreover, since, \\[ \\sum_{j=0}^s L_j(\\sigma) \\equiv 1, \\quad\\text{so that }\\quad\\sum_{j=0}^s \\dot L_j(\\sigma) \\equiv 0, \\] we have that \\[ \\sum_{j=0}^s \\dot L_j(\\gamma_m) =0= \\sum_{j=0}^s v_{mj} \\] and, thus, \\[ v_{m0} = -\\sum_{j=1}^s \\dot L_j(\\gamma_m) = -e_m^TVe. \\] With these relations we rewrite (6.7) as \\[ h\\dot X_{im} = \\sum_{\\ell=0}^sX_{i\\ell}\\dot L_\\ell(\\gamma_m) = v_{m0}x_i + \\sum_{\\ell=1}^sv_{m\\ell}X_{i\\ell}. \\] and \\(h\\sum_{m=1}^s\\alpha_{\\ell m} \\dot X_{im}\\) as \\[\\begin{align} h\\sum_{m=1}^s \\alpha_{\\ell m}\\dot X_{im} &amp;= \\sum_{m=1}^s \\alpha_{\\ell m}v_{m0}x_i + \\sum_{j,m=1}^s \\alpha_{\\ell m}v_{mj}X_{ij} \\notag \\\\ &amp;= -e_\\ell^T \\mathcal AV e x_i + \\sum_{j=1}^se_\\ell^T \\mathcal AVe_jX_{ij} \\tag{6.9}\\\\ &amp;= -x_i + X_{i\\ell}, \\notag \\end{align}\\] which, together with (6.8), indeed defines a RKM. Some remarks: the preceding derivation shows that the collocation (6.6) and (6.8) is equivalent to the RKM scheme (6.9) and (6.8) convergence of these schemes applied to (6.3)–(6.3) is proven in Kunkel/Mehrmann Theorem 5.17 with fixing \\(\\gamma_s=1\\), the obtained RKM is stiffly accurate the remaining \\(s-1\\) \\(\\gamma\\)s can be chosen to get optimal convergence rates \\(\\rightarrow\\) RadauIIa schemes if also \\(\\gamma_s\\) is chosen optimal in terms of convergence, the Gauss schemes are obtained "],["examples-1.html", "7 Examples 7.1 Semi-discrete Navier-Stokes equations", " 7 Examples 7.1 Semi-discrete Navier-Stokes equations By scalings and state transforms, we find that the coefficients of the spatially discretized Navier-Stokes equations are equivalent to: \\[\\begin{align*} (\\mathcal E, \\mathcal A) &amp;= \\left( \\begin{bmatrix} M &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} A &amp; B^H \\\\ B &amp; 0 \\end{bmatrix} \\right) \\\\ &amp; \\backsim \\left( \\begin{bmatrix} M^{-1/2} &amp; 0 \\\\ 0 &amp; I \\end{bmatrix} \\begin{bmatrix} M &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} A &amp; B^H \\\\ B &amp; 0 \\end{bmatrix} \\begin{bmatrix} M^{-1/2} &amp; 0 \\\\ 0 &amp; I \\end{bmatrix} \\right) \\\\ &amp; \\backsim \\left( \\begin{bmatrix} Q^H &amp; 0 \\\\ 0 &amp; I \\end{bmatrix} \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} M^{-1/2}AM^{-1/2} &amp; M^{-1/2}B^H \\\\ B M^{-1/2} &amp; 0 \\end{bmatrix} \\begin{bmatrix} Q &amp; 0 \\\\ 0 &amp; I \\end{bmatrix} \\right) \\\\ &amp; \\backsim \\left( \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp; R^{-H} \\end{bmatrix} \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix} M^{-1/2}AM^{-1/2} &amp; \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\\\ \\begin{bmatrix}R^H &amp; 0\\end{bmatrix} &amp; 0 \\end{bmatrix} \\begin{bmatrix} I &amp; 0 \\\\ 0 &amp; R^{-1} \\end{bmatrix} \\right) \\\\ &amp; \\quad = \\left( \\begin{bmatrix} I_{n_1} &amp; 0 &amp; 0 \\\\ 0 &amp; I_{n_2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{bmatrix} , \\begin{bmatrix} A_{11} &amp; A_{12} &amp; I_{n_1} \\\\ A_{21} &amp; A_{22} &amp; 0 \\\\ I_{n_1} &amp; 0 &amp; 0\\end{bmatrix} \\right). \\end{align*}\\] where we have used a QR-decomposition: \\[M^{-1/2}B^H=Q\\begin{bmatrix}R \\\\ 0\\end{bmatrix}\\] with unitary \\(Q\\) and invertible \\(R\\) in the third step. "],["exercises.html", "8 Exercises 8.1 II.C.1", " 8 Exercises 8.1 II.C.1 Let \\(E\\), \\(A \\in \\mathbb C^{n,n}\\) satisfy \\(EA=AE\\). Then \\(\\ker E \\cap \\ker A = \\{0\\}\\) implies that \\((E, A)\\) is regular. Assume that \\(\\ker A \\neq \\{0\\}\\) is of dimension \\(k\\geq 1\\). The case that \\(k=0\\) is trivial, since \\(\\lambda E - A\\) is regular for \\(\\lambda = 0\\). Let \\(V_0\\) be the matrix whose columns span \\(\\ker A\\) and let \\(V_\\perp\\) be the matrix that consists of all eigenvectors of \\(A\\) that are associated with the nonzero eigenvalues. It holds that, \\[AV_0 = 0 \\quad \\text{and} \\quad AV_\\perp = V_\\perp L_\\perp\\] with an \\(L_\\perp \\in C^{n-k,n-k}\\) which is invertible. This a consequence of \\(V_\\perp\\) spanning the \\(A\\)-invariant subspaces with respect to the nonzero eigenvalues. Because of \\(ABV_0=BAV_0=0\\), it follows that \\(\\operatorname{span}BV_0 \\subset \\ker A = \\operatorname{span}V_0\\), i.e., \\(V_0\\) is a \\(B\\)-invariant subspace which means that there is a \\(K_0\\in \\mathbb C^{k,k}\\) such that \\(BV_0 =V_0K_0\\). Moreover, because of \\(\\ker E \\cap \\ker A = \\{0\\}\\), the matrix \\(K_0\\) has no zero eigenvalues. In fact \\(K_0\\) has the same eigenvalues as \\(B&#39;:=B\\bigr|_{V_0}\\colon V_0 \\to V_0\\), and if \\(B&#39;\\) had a zero eigenvalue this would mean that the associated eigenvector would be in \\(V_0\\) and, thus, in the kernel of \\(A\\). Moreover, since \\(ABV_\\perp=BAV_\\perp=BVL_\\perp\\) meaning that \\(BV_\\perp\\) is in the \\(A\\)-invariant subspace related to the nonzero eigenvalues of \\(A\\), i.e., \\(BV_\\perp \\subset V_\\perp\\), it follows that \\(V_\\perp\\) is a \\(B\\)-invariant subspace and, thus, \\(BV_\\perp = V_\\perp K_\\perp\\) for some matrix \\(K_\\perp \\in \\mathbb C^{n-k,n-k}\\). With \\(V:=[V_0 |V_\\perp]\\) and the observation that \\(V\\) is invertible, since its columns span all of \\(\\mathbb C^n = \\operatorname{span}V_0 \\oplus \\operatorname{span}V_\\perp\\), it follows that \\[\\begin{equation*} \\begin{split} \\lambda E - A &amp; = (\\lambda E - A)VV^{-1} = (\\lambda E [V_0 |V_\\perp]- A[V_0 |V_\\perp])V^{-1} \\\\ &amp; = ([V_0 K_0 |V_\\perp K_\\perp]\\lambda - [0 |V_\\perp L_\\perp])V^{-1} \\\\ &amp; = [V_0 |V_\\perp ] \\begin{bmatrix} \\lambda K_0 &amp; \\\\ &amp; \\lambda K_\\perp - L_\\perp \\end{bmatrix} V^{-1} \\end{split} \\end{equation*}\\] and that \\[ \\det (\\lambda E - A) = \\det (\\lambda K_0) \\det(\\lambda K_\\perp - L_\\perp) \\] is not identically zero, since \\(K_0\\) and \\(L_\\perp\\) are invertible. "],["numerical-analysis-and-software-overview.html", "9 Numerical Analysis and Software Overview 9.1 Theory: RKMs and BDF for DAEs 9.2 Solvers 9.3 Software", " 9 Numerical Analysis and Software Overview 9.1 Theory: RKMs and BDF for DAEs Table 9.1: Overview of convergence results of RKM/BDF schemes for DAEs DAEs unstructured, linear \\(E(t)\\dot x = A(t)x + f(t)\\) semi-linear \\(E(t)\\dot x = f(t,x)\\) unstructured \\(F(t,\\dot x, x)=0\\) unstructured, strangeness-free/index-1 \\(\\begin{cases}\\hat F_1(t,\\dot x, x)=0 \\\\ \\hat F_2(t,x)=0 \\end{cases}\\) semi-explicit, strangeness-free/index-1 \\(\\begin{cases}\\dot x= f(t, x, y) \\\\ 0=g(t,x,y) \\end{cases}\\) semi-explicit, index-2 \\(\\begin{cases}\\dot x= f(t, x, y) \\\\ 0=g(t,y) \\end{cases}\\) Table 9.2: Overview of convergence results of BDF/RKM schemes for DAEs of various index and, possibly, semi-explicit structure. Here, we equate index-1 and strangeness-free. A \\(\\cdot\\) indicates that this case is included in a result for a more general case left or above in the table. RKM BDF unstructured semi-explicit unstructured semi-explicit Problem / Index \\(*\\) \\(2\\) \\(1\\) \\(*\\) \\(2\\) \\(1\\) \\(*\\) \\(2\\) \\(1\\) \\(*\\) \\(2\\) \\(1\\) nonlinear c g,i b f h e linear TV \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) linear CC a \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) d \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Description Reference a RKM, linear constant coefficients KM Thm. 5.12 b RKM, nonlinear, strangeness-free/index-1, semi-explicit KM Thm 5.16 / HW Thm. VI.1.1 c RKM, nonlinear, strangeness-free KM Thm. 5.18 d BDF, linear constant coefficients KM Thm. 5.24 e BDF(\\(\\subset\\) MSM), nonlinear, strangeness-free/index-1, semi-explicit KM Thm. 5.26 (\\(\\subset\\) HW Thm. VI.2.1) f BDF, nonlinear, strangeness-free/index-1 KM Thm. 5.27 g RKM, nonlinear, index-2, semi-explicit HW Ch. VII.4 h BDF, nonlinear, index-2, semi-explicit HW Thm. VII.3.5 i half-explicit RKM, nonlinear, index-2, semi-explicit HW Thm. VII.6.2 HW Ernst Hairer, Gerhard Wanner (1996) Solving ordinary differential equations. II: Stiff and differential-algebraic problems KM Peter Kunkel, Volker Mehrmann (2006) Differential-Algebraic Equations. Analysis and Numerical Solution 9.2 Solvers As can be seen from the table above, generally usable discretization methods for unstructured DAEs are only there for index-1 problems. However, the solvers GELDA/GENDA include an automated reduction to the strangeness-free form so that they apply for any index; see Lecture Chapter 4++. 9.2.1 Multi purpose DAEs Methods h/p Language Remark Avail GELDA l-\\(\\mu\\)-\\(*\\) BDF/RKM \\(*\\)/\\(*\\) F-77 \\(*\\)/\\(\\cdot\\) GENDA n-\\(\\mu\\)-\\(*\\) BDF \\(*\\)/\\(*\\) F-77 \\(\\phantom{*}\\)/\\(\\cdot\\) DASSL n-\\(\\nu\\)-\\(1\\) BDF \\(*\\)/\\(*\\) F-77 base for Sundials IDA – the base of many DAE solvers \\(*\\)/\\(\\phantom{\\cdot}\\) LIMEX sl-\\(\\nu\\)-\\(1\\) x-SE-Eul \\(*\\)/\\(*\\) F-77 \\(\\phantom{*}\\)/\\(\\phantom{\\cdot}\\) RADAU sl-\\(\\nu\\)-\\(1\\) RKM \\(*\\)/\\(*\\) F-77 \\(*\\)/\\(\\phantom{\\cdot}\\) Notes: Explanation DAEs l-linear, sl-semilinear, nl-nonlinear classification: \\(\\mu\\)-strangeness index, \\(\\nu\\)-differentiation index \\(*\\)-includes index reduction h/p time step control / order control availability code for download / licence provided(\\(*\\)) or other statement(\\(\\cdot\\)) methods x-SE-Eul: extrapolation based on semiexplicit Euler 9.2.2 Application specific Furthermore, there are solvers for particularly structured DAEs. DAEs Resources Navier-Stokes (nl-se-\\(2\\)) See, e.g., Sec. 4.3 of our preprint on definitions of different schemes Multi-Body (nl-se-\\(3\\)) See, e.g., the code on Hairer’s homepage 9.3 Software Many software suits actually wrap SUNDIALS IDA. DAEs Routines Method Remark Matlab ind-\\(1\\) ode15{i,s} BDF Python — no built-in functionality, DASSL/IDA wrapped in the modules scikit-odes, assimulo, pyDAS, DAEtools Julia ind-\\(1\\) DifferentialEquations.jl BDF calls SUNDIALS IDA "]]
