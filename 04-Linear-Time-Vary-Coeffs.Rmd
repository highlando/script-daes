\newcommand{\kernel}{\operatorname{kernel}}
\newcommand{\corange}{\operatorname{corange}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\cokernel}{\operatorname{cokernel}}

# Linear DAEs with Time-varying Coefficients {#IV}

In this section, we consider linear DAEs with *variable* or *time-dependent* coefficients. This means, for matrix-valued functions
$$
E \in \mathcal C(\mathcal I, \mathbb C^{m,n}), \quad A\in \mathcal C(\mathcal I, \mathbb C^{m,n})
$$
and $f\in \mathcal C(\mathcal I, \mathbb C^{m})$, we consider the DAE
\begin{equation}
E(t)\dot x(t) = A(t)x(t) + f(t) (\#eq:iv-ltv-dae)
\end{equation}
with, possibly, an initial condition
\begin{equation}
x(t_0) = x_0 \in \mathbb C^{n}. (\#eq:iv-ltv-inicond)
\end{equation}

```{block, type='JHSAYS'}
The same [general solution concept](#def:dae-solution) applies. Basically $x$ should be differentiable, fulfill the DAE, and, if stated, the initial condition too.
```

In the constant coefficient case, [regularity](III.html/#def:regularity) played a decisive role for the existence and uniqueness of solutions; see, e.g. [Section](#III-ex-sols) \@ref(III-ex-sols). Thus it seems natural to extend this concept to the time-varying case, e.g., through requiring that $(E(t), A(t))$ is a regular matrix pair independent of $t$. However, the following two examples show that this will not work *out of the box*.

```{example, label="ltv-regular-infinite-sols"}
Let $E$, $A$ be given as 
$$
E(t) = 
\begin{bmatrix}
-t & t^2 \\ -1 & t 
\end{bmatrix}, \quad
A(t) = 
\begin{bmatrix}
-1 & 0 \\ 0& -1
\end{bmatrix}
$$
Then
$$
\det ( \lambda E(t) - A(t)) = (1-\lambda t)(1+\lambda t) + \lambda ^2 t^2 \equiv 1,
$$
for all $t\in \mathcal I$. Still, for every $c \in \mathcal C^1(\mathcal I, \mathbb C)$ with $c(t_0)=0$, the function
$$
x\colon t \mapsto c(t)\begin{bmatrix} t\\1 \end{bmatrix}
$$
solves the *homogeneous* initial value problem \@ref(eq:iv-ltv-dae)--\@ref(eq:iv-ltv-inicond).
```

```{block, type='JHSAYS'}
This was an example where the pair $(E,A)$ is regular uniformly with respect to $t$ but still allows for infinitely many solutions to the associated DAE. **X**: What about the initial value? Why it won't help to make the solution unique?

Next we see the contrary -- a matrix pair that is singular for any $t$ but defines a unique solution.
```

```{example, label="ltv-singular-unique-sol"}
For 
$$
E(t) = 
\begin{bmatrix}
0 & 0 \\ 1 & -t 
\end{bmatrix}, \quad
A(t) = 
\begin{bmatrix}
-1 & t \\ 0&0 
\end{bmatrix}, \quad
f(t) = 
\begin{bmatrix}
f_1(t) \\ f_2(t)
\end{bmatrix}, 
$$
one has
$$
\det ( \lambda E(t) - A(t)) = 0
$$
for all $t\in \mathcal I$. Still, if $x=(x_1, x_2)$ denotes the solution, from the first line of the DAE
\begin{align*}
0 &= -x_1(t) + tx_2(t) + f_1(t) \\
\dot x_1 - t\dot x_2(t) &= \phantom{-x_1(t) + tx_2(t) +}f_2(t)
\end{align*}
one can calculate directly that
$$
\dot x_1(t) = t\dot x_2(t) +x_2 + \dot{f_1}(t)
$$
or that 
$$
\dot x_1(t) - t\dot x_2(t) = x_2 + \dot{f_1}(t)
$$
so that the second line becomes
$$
x_2(t) +  \dot{f_1}(t) = f_2(t)
$$
which uniquely defines 
$$
x_2(t) =  - \dot{f_1}(t) + f_2(t)
$$
and also
$$
x_1(t) =  - t(\dot{f_1}(t) + f_2(t))+f_1(t).
$$
```

```{block, type='JHSAYS'}
For both examples one can then simply choose $x(t_0)$ in accordance with the right hand side to argue about whether and how a solution exists. 
```

Recall that for the *constant coefficient* case, we were using invertible scaling and state transformation matrices $P$ and $Q$ for the equivalence transformations
$$
E \dot x(t) = Ax(t) +f(t) \quad \sim \quad \tilde E \dot {\tilde x(t)} = \tilde A\tilde x(t) +\tilde f(t) 
$$

with

$$
x=Q\tilde x, \quad \tilde E = PEQ, \quad \tilde A = PAQ, \quad \tilde f = Pf.
$$

For the time-varying case, we will use time-varying transformations and require that they are invertible at every point $t$ in time.

```{definition, label="global-equivalence"}
Two pairs $(E_i,A_i)$, $E_i$, $A_i \in \mathcal C(\mathcal I, \mathbb C^{m,n})$,
$i=1,2$, of matrix functions are called *(globally) equivalent*, if there exist
pointwise nonsingular matrix functions $P\in \mathcal C(\mathcal I, \mathbb
C^{m,m})$ and $Q\in \mathcal C^1(\mathcal I, \mathbb C^{n,n})$ such that
\begin{equation}
E_2=PE_1Q, \quad A_2 = PA_1Q-PE_1\dot Q (\#eq:iv-glob-equiv-mpairs)
\end{equation}
for all $t\in \mathcal I$. Again, we write $(E_1,A_1) \sim (E_2, A_2)$.
```

```{block, type='JHSAYS'}
The need of $Q$ being differentiable and the appearance of $E_1\dot Q$ in the definition of $A_2$ comes from the relation
$$
E\dot x(t) = E\frac{d}{dt} (Q\tilde x)(t) = E\bigl(Q(t)\dot {\tilde x}(t) + \dot Q(t)\tilde x(t) \bigr)
$$
for the transformed state $\tilde x$ with the actual state $x$.
```

```{lemma, label="iv-glob-equiv-rst"}
The relation on pairs of matrix functions as defined in Definition \@ref(def:global-equivalence) is an equivalence relation.
```

```{proof}
Exercise!
```

Next we will define *local* equivalence of matrix pairs.

```{definition, label="local-equivalence"}
Two pairs $(E_i,A_i)$, $E_i$, $A_i \in \mathbb C^{m,n}$,
$i=1,2$, of matrices are called *locally equivalent*, if there exist
pointwise nonsingular matrices $P \in \mathbb C^{m,m})$ and $Q\in \mathbb C^{n,n}$ such that as well as matrix $R\in \mathbb C^{n,n}$ such that
\begin{equation}
E_2=PE_1Q, \quad A_2 = PA_1Q-PE_1R (\#eq:iv-loc-equiv-mpairs).
\end{equation}
Again, we write $(E_1,A_1) \sim (E_2, A_2)$ and differentiate by context.
```

```{lemma, label="iv-loc-equiv-rst"}
The local equivalence as defined in Definition \@ref(def:local-equivalence) is an equivalence relation on pairs of matrices.
```

```{proof}
Exercise!
```

We state a few observations:

* Global equivalence implies local equivalence at all points of time $t$.
* Vice versa, pointwise local equivalence, e.g. at some time instances $t_i$ with suitable matrices $P_i$, $Q_i$, $R_i$, can be interpolated to a continuous matrix function $P$ and a differentiable matrix function $Q$ by *Hermite interpolation*, i.e. via 
  $$
  P(t_i) = P_i, \quad Q(t_i) = Q_i, \quad \dot Q(t_i) = R_i.
  $$
* Local equivalence is more powerful than the simple equivalence of matrix pairs (cp. Definition \@ref(def:matrix-pair-equivalent)) for which $R=0$. This means we can expect more structure in a normal form.

## A Local Canonical Form

For easier explanations, we introduce the slightly incorrect wording that a *matrix* $M$ *spans* a vector space $V$ to express that the $V$ is the span of the columns of $V$. Similarly, we will say that $M$ *is a basis of* $V$, if the columns of $M$ form a basis for $V$.

Some more notation:

| Notation | Explanation |
| -------- | ----------- |
| $V^H\in \mathbb C^{n,m}$ | the [*conjugate transpose* or *Hermitian transpose*](https://en.wikipedia.org/wiki/Conjugate_transpose)  of a matrix $V\in \mathbb C^{m,n}$ | 
| $T' \in \mathbb C^{n,n-k}$ | The *complementary space* as a matrix. If $T\in \mathbb C^{n,k}$ is a basis of $V$, then $T'$ contains a basis of $V'$ so that $V\oplus V' = \mathbb C^{n}$. In particular, the matrix $\begin{bmatrix}T&T'\end{bmatrix}$ is square and invertible. |

```{theorem, label="local-canonical-form"}
Let $E, A \in \mathbb C^{m,n}$ and let
\begin{equation}
T,~Z,~T',~V (\#eq:lcf-subspaces)
\end{equation}
be 

| Matrix | as the basis of |
|:-------|:----------------|
|$T$ | $\kernel E$ |
|$Z$ | $\corange E = \kernel E^H$ |
|$T'$ | $\cokernel E = \range E^H$ |
|$V$ | $\corange (Z^HAT)$ |

then the quantities
\begin{equation}
r,~a,~s,~d,~u,~v (\#eq:lcf-quantities)
\end{equation}
defined as 

| Quantity | Definition | Name |
|:-------|:-------------|:-----|
|$r$ | $\rank E$ | *rank* |
|$a$ | $\rank (Z^HAT)$ | *algebraic part* |
|$s$ | $\rank (V^HZ^HAT')$ | *strangeness* |
|$d$ | $r-s$ | *differential part* |
|$u$ | $n-r-a$ | *undetermined variables* |
|$v$ | $m-r-a-s$ | *vanishing equations* |

are invariant under local equivalence transformations and $(E, A)$ is locally equivalent to the canonical form


\begin{equation}
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & I_d & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 & 0 & 0  \\
0 & 0 & 0 & 0  \\
0 & 0 & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}\right),
(\#eq:local-canonical-form)
\end{equation}
where all diagonal blocks are square, except maybe the last one.
```

```{proof}
To be provided. Until then, see Theorem 3.7 in Kunkel/Mehrmann.
```
Some remarks on the spaces and how the names are derived for the case $E\dot x = Ax +f$ with constant coefficients. The ideas are readily transferred to the case with time-varying coefficients. 

Let 
$$x(t) = Ty(t) + T'y'(t),$$

where $y$ denotes the components of $x$ that evolve in the range of $T$ and $y'$ the respective complement. (Since $[T|T']$ is a basis of $\mathbb C^{n}$, there exist such $y$ and $y'$ that uniquely define $x$ and vice versa). With $T$ spanning $\ker E$ we find that 

$$
E \dot x(t) = ET\dot y(t) + ET'\dot y'(t) = ET'\dot y'(t)
$$

so that the DAE basically reads

$$ET'\dot y'(t) = ATy(t) + AT'y'(t)+f,$$

i.e. the components of $x$ defined through $y$ are, effectively, not differentiated. With $Z$ containing exactly those $v$, for which $v^HE=0$, it follows that 

$$Z^HET'\dot y'(t) = 0 = Z^HATy(t) + Z^HAT'y'(t)+Z^Hf,$$

or 

$$Z^HATy(t) = -Z^HAT'y'(t)-Z^Hf,$$

so that $\rank Z^HAT$ indeed describes the number of purely algebraic equations and variables in the sense that it defines parts of $y$ (which is never going to be differentiated) in terms of algebraic relations (no time derivatives are involved).

With the same arguments and with $V=\corange Z^HAT$, it follows that 

$$V^HZ^HAT'y'(t) = -V^HZ^HATy(t) -V^HZ^Hf=-V^HZ^Hf,$$

is the part of $E\dot x = Ax + f$ in which those components $y'$ that are also differentiated are algebraically equated to a right-hand side. This is the *strangeness* (rather in the sense of *skewness*) of DAEs that variables can be both differential and algebraic. Accordingly, $\rank V^HZ^HAT'$ describes the size of the skewness component. 

Finally, those variables that are neither *strange* nor purely algebraic, i.e. those that are differentiated but not defined algebraically, are the *differential* variables. There is no direct characterization of them, but one can calculate their number as $r-s$, which means number of differentiated minus number of *strange* variables.

```{block, type='JHSAYS'}
**Outlook**: If there is no strangeness, the DAE is called strangeness-free. Strangeness can be eliminated through iterated differentiation and substitution. The needed number of such iterations (that is independent of the the size $s$ of the *strange* block here) will define the strangeness index.
```


```{example, label="strangeness-in-the-circuit"}
With a basic state transformation 
$$
\begin{bmatrix}
\tilde x_1 \\ \tilde x_2 \\ \tilde x_3
\end{bmatrix}
= 
\begin{bmatrix}
x_3 - x_2 \\ x_2-x_1 \\ x_3
\end{bmatrix},
$$
one finds for the coefficients of Example \@ref(exm:the-circuit) that:
$$
(E, A) \backsim 
\left(
\begin{bmatrix} C & 0 & 0 \\ 0 & 0 &0 \\ 0 & 0 &0  \end{bmatrix}
,
\begin{bmatrix} 0 & \frac{1}{R} & 0 \\ -1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\right).
$$

We compute the subspaces as defined in \@ref(eq:lcf-subspaces):

| Matrix | as the basis of/computed as |
|:----------|:------------------------|
| $T=\begin{bmatrix} 0 \\ I_2 \end{bmatrix}$ | $\kernel\begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}$ |
| $Z=\begin{bmatrix} 0 \\ I_2 \end{bmatrix}$ | $\corange\begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}=\kernel\begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}^H$ |
| $T'=\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ | $\cokernel\begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}=\range\begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}^H$ |
| $Z^HAT=I_2$ | $\begin{bmatrix} 0 \\ I_2 \end{bmatrix}^H\begin{bmatrix} 0 & \frac{1}{R} & 0 \\ -1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 0 \\ I_2 \end{bmatrix}$ |
| $V=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$ | $\corange (Z^HAT) = \kernel I_2^H\phantom{\begin{bmatrix} 0 \\ I_1 \end{bmatrix}}$ |
| $V^HZ^HAT'=\begin{bmatrix} 0 \end{bmatrix}$ | $\begin{bmatrix} 0 \\ 0 \end{bmatrix}^H\begin{bmatrix} 0 \\ I_2 \end{bmatrix}^H\begin{bmatrix} 0 & \frac{1}{R} & 0 \\ -1 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 \\0 \\ 0 \end{bmatrix}$ |

and derive the quantities as defined in \@ref(eq:lcf-quantities):

| Name | Value | Derived from |
|:-----|:------|--------------|
| rank | $r=1$ | $\rank E = \rank \begin{bmatrix} C & 0 \\ 0 & 0_2 \end{bmatrix}$ |
| algebraic part | $a=2$ | $\rank Z^HAT = \rank I_2$ |
| strangeness | $s=0$ | $\rank V^HZ^HAT' = \rank \begin{bmatrix} 0\end{bmatrix}$ |
| differential part | $d=1$ | $d=r-s=1-0$ |
| undetermined variables | $u=0$ | $u=n-r-a=3-2-1$ |
| vanishing equations | $v=0$ | $v=m-r-a-s=3-2-1-0$ |

```

```{example, label="strangeness-in-the-nse"}

```{block, type='JHSAYS'}
For the semi-discrete linearized Navier-Stokes equations, the derivation of the *local characteristic quantities* is laid out in the [Example Section](#x-nse-local-char-vals).
```

```


## A Global Canonical Form {#IV-Global-Canonical-Form}

A few observations:

 * For a pair of $(E, A)$ of **matrix functions**, we can compute the characteristic values $r$, $a$, $s$, $d$ as in \@ref(eq:lcf-quantities) for any given $t\in \mathcal I$.
 * Thus, $r$, $a$, $s$, $d\colon \mathcal I \to \mathbb R$ are functions of time $t$.
 * We will assume that $r$, $a$, $s$, $d$ are constant in time:
   * Analysis will be enabled through a so-called smooth singular value decomposition (SVD -- see the following theorem) that applies for matrices of constant rank.
   * Smooth matrix functions have countably many jumps in the rank. The analysis can be performed on subintervals, where the rank of the matrices are constant.
   * In practice, typically, there are but a few jumps in the rank at somewhat particular but known time instances or circumstances. 

```{block, type='JHSAYS'}
About a few and known jumps in the rank: A change in the ranks means an instantaneous change in the model itself. In fact the characteristic values, like the number of purely algebraic equations, would change suddenly. An example is the activation of a switch in an electrical circuit or *switched systems* in general.
```

```{theorem, label="continuous-svd", name="see Kunkel/Mehrmann, Thm. 3.9"}

Let $E\in \mathcal C^\ell(I, \mathbb C^{m,n})$ with $\rank E(t)=r$ for all $t\in I$. Then there exist pointwise unitary (and, thus, nonsingular) matrix functions $U\in \mathcal C^\ell(I, \mathbb C^{m,m})$ and $V\in \mathcal C^\ell(I, \mathbb C^{n,n})$, such that 

$$
 U^HEV =
 \begin{bmatrix}
 \Sigma & 0 \\
 0 & 0
 \end{bmatrix}
$$
with pointwise nonsingular $\Sigma \in \mathcal C^l(I, \mathbb C^{r,r})$. 
```

```{theorem, label="global-canonical-form"}

Let $E, A \in \mathcal C^l(I, \mathbb C^{m,n})$ be sufficiently smooth and suppose that 
\begin{equation}
	r(t) = r, \quad a(t)=a, \quad s(t)=s (\#eq:glob-local-char-vals)
\end{equation}

for the local characteristic values of $(E(t), A(t))$. Then $(E, A)$ is globally equivalent to the canonical form
\begin{equation}
\left(
\begin{bmatrix}
I_s &  0  & 0 & 0 \\
0   & I_d & 0 & 0 \\
0   &  0  & 0 & 0 \\
0   &  0  & 0 & 0 \\
0   &  0  & 0 & 0
\end{bmatrix}, 
\begin{bmatrix}
0   & A_{12}&  0  & A_{14} \\
0   &   0   &  0  & A_{24} \\
0   &   0   & I_a & 0 \\
I_s &   0   &  0  & 0 \\
0   &   0   &  0  & 0
\end{bmatrix}
\right ).
(\#eq:glob-can-form)
\end{equation}

All entries are again matrix functions on $I$ and the last block column in both matrix functions of \@ref(eq:glob-can-form) has size $u=n-s-d-a$.

```

```{proof}
In what follows, we will tacitly redefine the block matrix entries that appear after the global equivalence transformations. The first step is the continous SVD of $E$; see Theorem \@ref(thm:global-canonical-form). In what follows, the basic operations of 

 * condensing blocks by the continuous SVD, e.g. $U_2^HA_{31}V_2=\begin{bmatrix} I_s & 0 \\ 0 & 0 \end{bmatrix}$
 * and eliminating blocks through by adding multiples of columns or rows

are applied repeatedly:

<!-- \begin{ofalltheseblockmats} -->
\begin{align*}
(E,A) & 
\sim   
\left(\begin{bmatrix}
\Sigma & 0 \\
0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}\right) \\
%%%%%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_r & 0 \\
0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}\right) \\
%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_r & 0 \\
0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11} & A_{12}V_1 \\
U_1^HA_{21} & U_1^HA_{22}V_1 
\end{bmatrix}
-
\begin{bmatrix} I_r & 0 \\ 0 & 0 \end{bmatrix}
\begin{bmatrix} 0 & 0 \\ 0 & \dot V_1 \end{bmatrix}
\right) \\
%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_r & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11} & A_{12} & A_{13}\\
A_{21} & I_a & 0 \\
A_{31} & 0  & 0
\end{bmatrix}\right) \\
%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
V_2 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11}V_2 & A_{12} & A_{13}\\
A_{21}V_2 & I_a & 0 \\
U_2^HA_{31}V_2 & 0 & 0
\end{bmatrix}
-
\begin{bmatrix}
\dot I_r & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
\dot V_2 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix} \right)\\
%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
V_{11} & V_{12} & 0 & 0 \\
V_{21} & V_{22} & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & A_{14}  \\
A_{21} & A_{22} & A_{23} & A_{24}  \\
A_{31} & A_{32} & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix} \right)\\
%%%%%%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & I_d & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & A_{12} & A_{13} & A_{14}  \\
0 & A_{22} & A_{23} & A_{24}  \\
0 & A_{32} & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix} 
\begin{bmatrix}
I_s & 0 & 0 & 0  \\
0 & I_d & 0 & 0  \\
0 & -A_{32} & I_a & 0 \\
I_s & 0 & 0 & I_a
\end{bmatrix} 
\right) \\
%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & I_d & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & A_{12} & A_{13} & A_{14}  \\
0 & A_{22} & A_{23} & A_{24}  \\
0 & 0 & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}\right)\\
%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & I_d & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & A_{12} & 0 & A_{14}  \\
0 & A_{22} & 0 & A_{24}  \\
0 & 0 & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}\right)\\
%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & Q_2 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & A_{12}Q_2 & 0 & A_{14}  \\
0 & A_{22}Q_2-\dot Q_2 & 0 & A_{24}  \\
0 & 0 & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}\right) \\
%%%%%%%%%%%%%%%%%
& \sim   
\left(\begin{bmatrix}
I_s & 0 & 0 & 0 \\
0 & I_d & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & A_{12} & 0 & A_{14}  \\
0 & 0 & 0 & A_{24}  \\
0 & 0 & I_a & 0 \\
I_s & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}\right),
\end{align*}
<!-- \end{ofalltheseblockmats} -->
where the final equivalence holds, if $Q_2$ is chosen as the (unique and pointwise invertible) solution of the linear matrix valued ODE
$$
\dot Q_2 = A_{22}(t)Q_2 ,  \quad Q_2 (t_0 ) = I_d.
$$
Then, the prefinal $A_{22}$-block vanishes because of the special choice of $Q_2$ and $E_{22}$ becomes $I_d$ after scaling the second block line by $Q_2^{-1}$.
```

If we write down the transformed DAE that corresponds to the canonical form \@ref(eq:glob-can-form), i.e.
\begin{align}
\dot x_1 &= A_{12}(t)x_2 + A_{14}x_4 + f_1(t) (\#eq:iv-gcf-spart-1) \\
\dot x_2 &= A_{24}(t)x_4 + f_2(t) (\#eq:iv-gcf-dpart)\\
0 &= x_3 + f_3(t) \\
0 &= x_1 + f_4(t) (\#eq:iv-gcf-spart-2) \\
0 &= f_5(t) 
\end{align}
we can read off a few properties:

1. the part $x_4$ is *free to choose*, i.e. the undetermined part
2. the equation $f_5=0$ does not define any variable, i.e. it is the vanishing or redundant part
3. the part $x_2$ is defined through an ODE (in this representation)
4. **however**, the part $x_1$ is *strange* (both differential and algebraic) and still linked to $x_2$.

```{corollary, label='iv-diff-elim'}
In fact, one may **differentiate** \@ref(eq:iv-gcf-spart-2) and **eliminate** $\dot x_1$ in \@ref(eq:iv-gcf-spart-1) to obtain
$$
-\dot f_4 = A_{12}(t)x_2 + A_{14}x_4 + f_1(t)
$$
which together with \@ref(eq:iv-gcf-dpart) becomes a new DAE for $x_2$:
$$
\bar E(t) \dot x_2 = \bar A(t) x_2 + \bar f(t)
$$
with
\begin{equation}
\bar E(t) = 
\begin{bmatrix}
I_{d_0} \\ 0_{s_0, d_0} 
\end{bmatrix}\in \mathbb C^{d_0+s_0, d_0}, \quad
\bar A(t) =
\begin{bmatrix}
0_{d_0} \\ A_{12}(t)
\end{bmatrix} \in \mathbb C^{d_0+s_0, d_0}, \quad (\#eq:iv-gcf-post-diff)
\end{equation}
and
$$
\bar f(t) =
\begin{bmatrix}
A_{24}x_4(t) + f_2(t) \\ A_{14}x_4(t)+f_1(t)-\dot f_4(t)
\end{bmatrix} \in \mathbb C^{d_0+s_0}.
$$
Here, we have used the subscript to note that these $d$ and $s$ quantities were characteristic for the initial matrix pair $(E, A)$. Now, after this differentiation step, one can calculate the characteristic values $d_1$, $a_1$, $s_1$ again for the pair $(E_1, A_1)$ which is obtained from the canonical form of Theorem \@ref(eq:glob-can-form) by replacing equations \@ref(eq:iv-gcf-spart-1)--\@ref(eq:iv-gcf-dpart) by the DAE with $(\bar E, \bar A)$ as in \@ref(eq:iv-gcf-post-diff).
```

## Don't read any further

The following theorem states that this *differentiation-elimination* step 
(which is **not** a global equivalence operation on matrix pairs) is well-defined in the sense that the *next iteration* characteristic values are invariant under global equivalence transformations.

```{theorem,  label='iv-diff-elim-invariant'}
Assume that the pairs $(E, A)$ and $(\tilde E, \tilde A)$ are globally equivalent and in the global canonical form \@ref(eq:glob-can-form). Then the pairs $(E_1, A_1)$ and $(\tilde E_1, \tilde A_1)$ that are obtained by differentiation and elimination as described in Corollary \@ref(cor:iv-diff-elim) are globally equivalent too. 
```

```{proof}
See Kunkel/Mehrmann: Theorem 3.14.
```

Theorem \@ref(thm:iv-diff-elim-invariant) comes with a number of implications:

 * Starting with $(E, A):=(E_0, A_0)$, we can define $(E_i, A_i)$, $i \in \mathbb N \cup {0}$ as follows
   1. $(E_i, A_i)$ is the global canonical form
   2. differentiate and eliminate as in Corollary \@ref(cor:iv-diff-elim) and bring the obtained pair into global canonical form to obtain $(E_{i+1}, A_{i+1})$.
 * this gives a series of invariants $(r_i, a_i, s_i)$ -- invariant under global equivalence transforms --
 * Since $r_{i+1} = r_i - s_i$ and $r_i \geq 0$ (rank of a matrix is always greater than zero) there exists a $\mu \in \mathbb N \cup \{0\}$ for which $s_\mu = 0$.
 * This $\mu$ is also characteristic for a matrix pair (because $r_i$ and $s_i$ are).

